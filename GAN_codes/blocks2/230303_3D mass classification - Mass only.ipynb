{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# image data\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import sklearn #필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus =tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus: \n",
    "#     # tensorflow가 첫번째 gpu만 사용하도록 제한\n",
    "#     try: \n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#     except RuntimeError as e: \n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.is_gpu_available()\n",
    "#tf.test.is_built_with_cuda()\n",
    "tf.test.is_built_with_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization - only mass\n",
    "lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/ART/02_01_0001_*')\n",
    "\n",
    "test = Image.open(lst[0])\n",
    "plt.figure(figsize = (10, 10))\n",
    "for i in range(1, len(lst)): \n",
    "    tmp = Image.open(lst[i])\n",
    "    test = np.concatenate((test, tmp), axis = 1)\n",
    "\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization - only mass\n",
    "lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Train_data/ART/02_01_0002_*')\n",
    "\n",
    "test = Image.open(lst[0])\n",
    "plt.figure(figsize = (10, 10))\n",
    "for i in range(1, len(lst)): \n",
    "    tmp = Image.open(lst[i])\n",
    "    test = np.concatenate((test, tmp), axis = 1)\n",
    "    \n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization - only mass\n",
    "lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Train_data/ART/02_01_0003_*')\n",
    "\n",
    "test = Image.open(lst[0])\n",
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(1, len(lst)): \n",
    "    tmp = Image.open(lst[i])\n",
    "    test = np.concatenate((test, tmp), axis = 1)\n",
    "\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualization - only mass - ppl 1 from 9\n",
    "\n",
    "for idx in range(1, 10): \n",
    "    lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Train_data/ART/02_01_000{}_*'.format(idx))\n",
    "\n",
    "    test = Image.open(lst[0])\n",
    "    plt.figure(figsize = (20, 20))\n",
    "    for i in range(1, len(lst)): \n",
    "        tmp = Image.open(lst[i])\n",
    "        test = np.concatenate((test, tmp), axis = 1)\n",
    "        \n",
    "    print(\"patient index: \", idx)\n",
    "    plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualization - only mass - ppl 10 from 30\n",
    "\n",
    "for idx in range(10, 31): \n",
    "    lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Train_data/ART/02_01_00{}_*'.format(idx))\n",
    "\n",
    "    test = Image.open(lst[0])\n",
    "    plt.figure(figsize = (20, 20))\n",
    "    for i in range(1, len(lst)): \n",
    "        tmp = Image.open(lst[i])\n",
    "        test = np.concatenate((test, tmp), axis = 1)\n",
    "        \n",
    "    print(\"patient index: \", idx)\n",
    "    plt.imshow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pd.DataFrame(os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/ART/'))\n",
    "patient['id'] = patient[0].str[0:10]\n",
    "tmp = patient.groupby(['id']).count().sort_values([0], ascending = False)\n",
    "tmp[[0]].value_counts()\n",
    "# min 1\n",
    "# max 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pd.DataFrame(os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Train_data/PRE/'))\n",
    "patient['id'] = patient[0].str[0:10]\n",
    "patient.groupby(['id']).count().sort_values([0], ascending = False)\n",
    "# min 1\n",
    "# max 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pd.DataFrame(os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Test_data/ART/'))\n",
    "patient['id'] = patient[0].str[0:10]\n",
    "patient.groupby(['id']).count().sort_values([0], ascending = False)\n",
    "# min 2\n",
    "# max 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pd.DataFrame(os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_mass/Test_data/PRE/'))\n",
    "patient['id'] = patient[0].str[0:10]\n",
    "patient.groupby(['id']).count().sort_values([0], ascending = False)\n",
    "# min 2\n",
    "# max 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCM header check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm header check\n",
    "import pydicom\n",
    "\n",
    "path__ = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/1.Training/원천데이터/02_01_0001/02_01_0001_ART/0001.dcm'\n",
    "header_01 = pydicom.dcmread(path__, stop_before_pixels = True)\n",
    "header_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['name', 'modality'])\n",
    "data = [{'name' : '0', 'modality': str(header_01[0x0008, 0x0001030][0:])}]\n",
    "tmp = pd.DataFrame(data)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm header별 modality list\n",
    "path = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/1.Training/원천데이터/'\n",
    "patient = os.listdir(path)\n",
    "df = pd.DataFrame(columns = ['name', 'modality'])\n",
    "lst = []\n",
    "\n",
    "for ppl in patient: \n",
    "    deep_path = path + ppl + '/' + '{}_ART'.format(ppl) + '/'\n",
    "    filelist = os.listdir(deep_path)\n",
    "    for file in filelist: \n",
    "        try: \n",
    "            header = pydicom.dcmread(deep_path + file, stop_before_pixels = True)\n",
    "            data = [{'name' : ppl, 'modality': str(header[0x0008, 0x0001030][0:])}]\n",
    "            tmp = pd.DataFrame(data)\n",
    "            df = pd.concat([df, tmp], axis = 0)\n",
    "        except Exception: \n",
    "            lst.append(ppl)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['modality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = df.groupby(['modality']).count().reset_index()\n",
    "df_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modality_kidney_list = df.loc[df['modality'].isin(['CT Kidney (3P) + 3D', 'CT Kidney (3P) + 3D (contrast)']) == True, 'name'].unique()\n",
    "len(modality_kidney_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modality_kidney_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_pelvis_list = df.loc[df['modality'].isin(['Pelvis^00_Kidney_3D (Adult)']) == True, 'name'].unique()\n",
    "len(modality_pelvis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 사람별 ct 이미지 장수 확인\n",
    "\n",
    "n_list = []\n",
    "\n",
    "for ppl in modality_kidney_list: \n",
    "    lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing/TRAIN/ART/{}_*'.format(ppl))\n",
    "    lst.sort()\n",
    "    n_list.append(len(lst))\n",
    "    print(ppl, \"N: \", len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 사람별 ct 이미지 장수 확인\n",
    "\n",
    "n_list = []\n",
    "\n",
    "for ppl in modality_pelvis_list: \n",
    "    lst = glob('/home/ncp/workspace/blocks1/kidneyData_windowing/TRAIN/ART/{}_*'.format(ppl))\n",
    "    lst.sort()\n",
    "    n_list.append(len(lst))\n",
    "    print(ppl, \"N: \", len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = pd.DataFrame(n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0장인 사람이?\n",
    "# 0drop후 describe\n",
    "\n",
    "n_list = n_list.loc[n_list[0] != 0].reset_index(drop = True)\n",
    "n_list[0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 데이터 쓸 때 실행\n",
    "# # abnormal : RCC (악성)\n",
    "# # normal : aml + onco (양성)\n",
    "\n",
    "# ART_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/ART/'\n",
    "# ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "# PRE_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/PRE/'\n",
    "# PRE_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/PRE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass 만 쓸 때 실행 - mass 데이터를 악성 양성으로 분류해서 저장한 경로 참조\n",
    "# abnormal : RCC (악성)\n",
    "# normal : aml + onco (양성)\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TEST/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TRAIN/PRE/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TEST/PRE/'\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TRAIN/GCCT/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TEST/GCCT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scan(path): \n",
    "    # read scan\n",
    "    volume = Image.open(path).convert('L')\n",
    "    volume = np.array(volume)\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 데이터용\n",
    "\n",
    "# def padding_stacking(data_path, cancer_type): \n",
    "#     ## create distinct list of patients ##\n",
    "#     file_list = os.listdir(data_path + cancer_type + '/')\n",
    "#     patient = []\n",
    "#     for items in file_list: \n",
    "#         patient.append(items[0:10])\n",
    "\n",
    "#     patient = list(set(patient))\n",
    "#     print(\"total distinct patient N: \", len(patient))\n",
    "\n",
    "#     ## create empty datasets ##\n",
    "#     scans_final = np.zeros((512, 512, 128))\n",
    "#     ppl_scan_list = []\n",
    "\n",
    "#     ## iterate through distinct patient list and process ##\n",
    "#     for ppl in patient: \n",
    "\n",
    "#         ## 사람별로 path list 생성, path list에서 ct 불러와서 stack ## \n",
    "#         ## (512, 512, n) 장의 file 생성됨 ##\n",
    "#         ppl_path = []\n",
    "#         ppl_scan = np.zeros((512, 512))\n",
    "#         for x in os.listdir(data_path + cancer_type + '/'): \n",
    "#             if x[0:10] == ppl: \n",
    "#                 ppl_path.append(x)\n",
    "#         for path in ppl_path: \n",
    "#             ppl_scan = np.dstack((ppl_scan, process_scan(data_path + cancer_type + '/' + path)))  # one layer of zero padding added on top\n",
    "\n",
    "#         print(\"patient id: \", ppl)\n",
    "#         print(\"ppl_scan shape: \", ppl_scan.shape)\n",
    "\n",
    "#         ## zero pad to (512, 512, 128) ##\n",
    "#         height, width, depth = ppl_scan.shape\n",
    "#         pad_len = (128-depth)//2 # 양쪽에 padding 할 length 정의\n",
    "\n",
    "#         if depth >= 128: # if depth >= 128 then truncate\n",
    "#             if depth % 2 == 0: \n",
    "#                 pad_len = (depth-128)//2\n",
    "#                 ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len]\n",
    "#                 print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "#             else: \n",
    "#                 pad_len = (depth-128)//2\n",
    "#                 ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len-1]\n",
    "#                 print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "#         else: # if depth < 128 than pad\n",
    "#             if depth % 2 == 0: \n",
    "#                 ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len)), 'constant')\n",
    "#                 print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "#             else: \n",
    "#                 ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len+1)), 'constant')\n",
    "#                 print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "\n",
    "#         ppl_scan_list.append(ppl_scan_padded) # padding 완료된 file을 ppl_scan_list 에 저장\n",
    "#         print(\"---------------------\") \n",
    "\n",
    "#     # 사람별 생성 및 process 된 ppl_scan을 순회하면서 (N, 512, 512, 128) 로 저장 ##\n",
    "#     scans_final = np.array([ppl_scan_list[i] for i in range(len(ppl_scan_list))])\n",
    "#     print(\"scans_final shape: \", scans_final.shape) \n",
    "    \n",
    "#     return scans_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass 데이터용 - PADDING 64\n",
    "\n",
    "def padding_stacking(data_path, cancer_type): \n",
    "    ## create distinct list of patients ##\n",
    "    file_list = os.listdir(data_path + cancer_type + '/')\n",
    "    patient = []\n",
    "    for items in file_list: \n",
    "        patient.append(items[0:10])\n",
    "\n",
    "    patient = list(set(patient))\n",
    "    print(\"total distinct patient N: \", len(patient))\n",
    "\n",
    "    ## create empty datasets ##\n",
    "    scans_final = np.zeros((512, 512, 64))\n",
    "    ppl_scan_list = []\n",
    "\n",
    "    ## iterate through distinct patient list and process ##\n",
    "    for ppl in patient: \n",
    "\n",
    "        ## 사람별로 path list 생성, path list에서 ct 불러와서 stack ## \n",
    "        ## (512, 512, n) 장의 file 생성됨 ##\n",
    "        ppl_path = []\n",
    "        ppl_scan = np.zeros((512, 512))\n",
    "        for x in os.listdir(data_path + cancer_type + '/'): \n",
    "            if x[0:10] == ppl: \n",
    "                ppl_path.append(x)\n",
    "        for path in ppl_path: \n",
    "            ppl_scan = np.dstack((ppl_scan, process_scan(data_path + cancer_type + '/' + path)))  # one layer of zero padding added on top\n",
    "\n",
    "        print(\"patient id: \", ppl)\n",
    "        print(\"ppl_scan shape: \", ppl_scan.shape)\n",
    "\n",
    "        ## zero pad to (512, 512, 64) ##\n",
    "        height, width, depth = ppl_scan.shape\n",
    "        pad_len = (64-depth)//2 # 양쪽에 padding 할 length 정의\n",
    "\n",
    "        if depth >= 64: # if depth >= 64 then truncate\n",
    "            if depth % 2 == 0: \n",
    "                pad_len = (depth-64)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                pad_len = (depth-64)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len-1]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "        else: # if depth < 64 than pad\n",
    "            if depth % 2 == 0: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len+1)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "\n",
    "        ppl_scan_list.append(ppl_scan_padded) # padding 완료된 file을 ppl_scan_list 에 저장\n",
    "        print(\"---------------------\") \n",
    "\n",
    "    # 사람별 생성 및 process 된 ppl_scan을 순회하면서 (N, 512, 512, 64) 로 저장 ##\n",
    "    scans_final = np.array([ppl_scan_list[i] for i in range(len(ppl_scan_list))])\n",
    "    print(\"scans_final shape: \", scans_final.shape) \n",
    "    \n",
    "    return scans_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass 데이터용 - PADDING 32\n",
    "\n",
    "def padding_stacking(data_path, cancer_type): \n",
    "    ## create distinct list of patients ##\n",
    "    file_list = os.listdir(data_path + cancer_type + '/')\n",
    "    patient = []\n",
    "    for items in file_list: \n",
    "        patient.append(items[0:10])\n",
    "\n",
    "    patient = list(set(patient))\n",
    "    print(\"total distinct patient N: \", len(patient))\n",
    "\n",
    "    ## create empty datasets ##\n",
    "    scans_final = np.zeros((512, 512, 32))\n",
    "    ppl_scan_list = []\n",
    "\n",
    "    ## iterate through distinct patient list and process ##\n",
    "    for ppl in patient: \n",
    "\n",
    "        ## 사람별로 path list 생성, path list에서 ct 불러와서 stack ## \n",
    "        ## (512, 512, n) 장의 file 생성됨 ##\n",
    "        ppl_path = []\n",
    "        ppl_scan = np.zeros((512, 512))\n",
    "        for x in os.listdir(data_path + cancer_type + '/'): \n",
    "            if x[0:10] == ppl: \n",
    "                ppl_path.append(x)\n",
    "        for path in ppl_path: \n",
    "            ppl_scan = np.dstack((ppl_scan, process_scan(data_path + cancer_type + '/' + path)))  # one layer of zero padding added on top\n",
    "\n",
    "        print(\"patient id: \", ppl)\n",
    "        print(\"ppl_scan shape: \", ppl_scan.shape)\n",
    "\n",
    "        ## zero pad to (512, 512, 32) ##\n",
    "        height, width, depth = ppl_scan.shape\n",
    "        pad_len = (32-depth)//2 # 양쪽에 padding 할 length 정의\n",
    "\n",
    "        if depth >= 32: # if depth >= 32 then truncate\n",
    "            if depth % 2 == 0: \n",
    "                pad_len = (depth-32)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                pad_len = (depth-32)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len-1]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "        else: # if depth < 32 than pad\n",
    "            if depth % 2 == 0: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len+1)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "\n",
    "        ppl_scan_list.append(ppl_scan_padded) # padding 완료된 file을 ppl_scan_list 에 저장\n",
    "        print(\"---------------------\") \n",
    "\n",
    "    # 사람별 생성 및 process 된 ppl_scan을 순회하면서 (N, 512, 512, 32) 로 저장 ##\n",
    "    scans_final = np.array([ppl_scan_list[i] for i in range(len(ppl_scan_list))])\n",
    "    print(\"scans_final shape: \", scans_final.shape) \n",
    "    \n",
    "    return scans_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass 데이터용 - PADDING 16\n",
    "\n",
    "def padding_stacking(data_path, cancer_type): \n",
    "    ## create distinct list of patients ##\n",
    "    file_list = os.listdir(data_path + cancer_type + '/')\n",
    "    patient = []\n",
    "    for items in file_list: \n",
    "        patient.append(items[0:10])\n",
    "\n",
    "    patient = list(set(patient))\n",
    "    print(\"total distinct patient N: \", len(patient))\n",
    "\n",
    "    ## create empty datasets ##\n",
    "    scans_final = np.zeros((512, 512, 16))\n",
    "    ppl_scan_list = []\n",
    "\n",
    "    ## iterate through distinct patient list and process ##\n",
    "    for ppl in patient: \n",
    "\n",
    "        ## 사람별로 path list 생성, path list에서 ct 불러와서 stack ## \n",
    "        ## (512, 512, n) 장의 file 생성됨 ##\n",
    "        ppl_path = []\n",
    "        ppl_scan = np.zeros((512, 512))\n",
    "        for x in os.listdir(data_path + cancer_type + '/'): \n",
    "            if x[0:10] == ppl: \n",
    "                ppl_path.append(x)\n",
    "        for path in ppl_path: \n",
    "            ppl_scan = np.dstack((ppl_scan, process_scan(data_path + cancer_type + '/' + path)))  # one layer of zero padding added on top\n",
    "\n",
    "        print(\"patient id: \", ppl)\n",
    "        print(\"ppl_scan shape: \", ppl_scan.shape)\n",
    "\n",
    "        ## zero pad to (512, 512, 16) ##\n",
    "        height, width, depth = ppl_scan.shape\n",
    "        pad_len = (16-depth)//2 # 양쪽에 padding 할 length 정의\n",
    "\n",
    "        if depth >= 16: # if depth >= 16 then truncate\n",
    "            if depth % 2 == 0: \n",
    "                pad_len = (depth-16)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                pad_len = (depth-16)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len-1]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "        else: # if depth < 16 than pad\n",
    "            if depth % 2 == 0: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len+1)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "\n",
    "        ppl_scan_list.append(ppl_scan_padded) # padding 완료된 file을 ppl_scan_list 에 저장\n",
    "        print(\"---------------------\") \n",
    "\n",
    "    # 사람별 생성 및 process 된 ppl_scan을 순회하면서 (N, 512, 512, 16) 로 저장 ##\n",
    "    scans_final = np.array([ppl_scan_list[i] for i in range(len(ppl_scan_list))])\n",
    "    print(\"scans_final shape: \", scans_final.shape) \n",
    "    \n",
    "    return scans_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass 데이터용 - PADDING 8\n",
    "\n",
    "def padding_stacking(data_path, cancer_type): \n",
    "    ## create distinct list of patients ##\n",
    "    file_list = os.listdir(data_path + cancer_type + '/')\n",
    "    patient = []\n",
    "    for items in file_list: \n",
    "        patient.append(items[0:10])\n",
    "\n",
    "    patient = list(set(patient))\n",
    "    print(\"total distinct patient N: \", len(patient))\n",
    "\n",
    "    ## create empty datasets ##\n",
    "    scans_final = np.zeros((512, 512, 8))\n",
    "    ppl_scan_list = []\n",
    "\n",
    "    ## iterate through distinct patient list and process ##\n",
    "    for ppl in patient: \n",
    "\n",
    "        ## 사람별로 path list 생성, path list에서 ct 불러와서 stack ## \n",
    "        ## (512, 512, n) 장의 file 생성됨 ##\n",
    "        ppl_path = []\n",
    "        ppl_scan = np.zeros((512, 512))\n",
    "        for x in os.listdir(data_path + cancer_type + '/'): \n",
    "            if x[0:10] == ppl: \n",
    "                ppl_path.append(x)\n",
    "        for path in ppl_path: \n",
    "            ppl_scan = np.dstack((ppl_scan, process_scan(data_path + cancer_type + '/' + path)))  # one layer of zero padding added on top\n",
    "\n",
    "        print(\"patient id: \", ppl)\n",
    "        print(\"ppl_scan shape: \", ppl_scan.shape)\n",
    "\n",
    "        ## zero pad to (512, 512, 8) ##\n",
    "        height, width, depth = ppl_scan.shape\n",
    "        pad_len = (8-depth)//2 # 양쪽에 padding 할 length 정의\n",
    "\n",
    "        if depth >= 8: # if depth >= 8 then truncate\n",
    "            if depth % 2 == 0: \n",
    "                pad_len = (depth-8)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                pad_len = (depth-8)//2\n",
    "                ppl_scan_padded = ppl_scan[:, :, pad_len:depth-pad_len-1]\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "        else: # if depth < 8 than pad\n",
    "            if depth % 2 == 0: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "            else: \n",
    "                ppl_scan_padded = np.pad(ppl_scan, ((0,0), (0,0), (pad_len, pad_len+1)), 'constant')\n",
    "                print(\"padded ppl_scan shape: \", ppl_scan_padded.shape)\n",
    "\n",
    "        ppl_scan_list.append(ppl_scan_padded) # padding 완료된 file을 ppl_scan_list 에 저장\n",
    "        print(\"---------------------\") \n",
    "\n",
    "    # 사람별 생성 및 process 된 ppl_scan을 순회하면서 (N, 512, 512, 16) 로 저장 ##\n",
    "    scans_final = np.array([ppl_scan_list[i] for i in range(len(ppl_scan_list))])\n",
    "    print(\"scans_final shape: \", scans_final.shape) \n",
    "    \n",
    "    return scans_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART train : RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ART_train_abnormal_scans = padding_stacking(ART_train_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART train: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ART_train_normal_scans = padding_stacking(ART_train_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART test: RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ART_test_abnormal_scans = padding_stacking(ART_test_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART test: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ART_test_normal_scans = padding_stacking(ART_test_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE train: RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRE_train_abnormal_scans = padding_stacking(PRE_train_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE train: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRE_train_normal_scans = padding_stacking(PRE_train_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE test: RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_test_abnormal_scans = padding_stacking(PRE_test_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE test: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRE_test_normal_scans = padding_stacking(PRE_test_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCCT train: RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCCT_train_abnormal_scans = padding_stacking(GCCT_train_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCCT train: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GCCT_train_normal_scans = padding_stacking(GCCT_train_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCCT test: RCC (abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCCT_test_abnormal_scans = padding_stacking(GCCT_test_path, 'RCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCCT test: AML + onco (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GCCT_test_normal_scans = padding_stacking(GCCT_test_path, 'AML + onco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ART\n",
    "print(\"ART_train_abnormal_scans shape: \", ART_train_abnormal_scans.shape)\n",
    "print(\"ART_train_normal_scans shape: \", ART_train_normal_scans.shape)\n",
    "print(\"ART_test_abnormal_scans shape: \", ART_test_abnormal_scans.shape)\n",
    "print(\"ART_test_normal_scans shape: \", ART_test_normal_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE\n",
    "print(\"PRE_train_abnormal_scans shape: \", PRE_train_abnormal_scans.shape)\n",
    "print(\"PRE_train_normal_scans shape: \", PRE_train_normal_scans.shape)\n",
    "print(\"PRE_test_abnormal_scans shape: \", PRE_test_abnormal_scans.shape)\n",
    "print(\"PRE_test_normal_scans shape: \", PRE_test_normal_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCCT\n",
    "print(\"GCCT_train_abnormal_scans shape: \", GCCT_train_abnormal_scans.shape)\n",
    "print(\"GCCT_train_normal_scans shape: \", GCCT_train_normal_scans.shape)\n",
    "print(\"GCCT_test_abnormal_scans shape: \", GCCT_test_abnormal_scans.shape)\n",
    "print(\"GCCT_test_normal_scans shape: \", GCCT_test_normal_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 set끼리만 build 해야 함!! (ART, PRE, GCCT 각각) (메모리 에러 남)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ART\n",
    "\n",
    "ART_train_abnormal_labels = np.array([1 for _ in range(len(ART_train_abnormal_scans))])\n",
    "ART_train_normal_labels = np.array([0 for _ in range(len(ART_train_normal_scans))])\n",
    "\n",
    "# split 7:3 for validation\n",
    "abnormal_train_idx = round(len(ART_train_abnormal_labels) * 0.7)\n",
    "normal_train_idx = round(len(ART_train_normal_labels) * 0.7)\n",
    "\n",
    "x_train = np.concatenate((ART_train_abnormal_scans[:abnormal_train_idx], ART_train_normal_scans[:normal_train_idx]), axis = 0)\n",
    "y_train = np.concatenate((ART_train_abnormal_labels[:abnormal_train_idx], ART_train_normal_labels[:normal_train_idx]), axis = 0)\n",
    "\n",
    "x_val = np.concatenate((ART_train_abnormal_scans[abnormal_train_idx:], ART_train_normal_scans[normal_train_idx:]), axis = 0)\n",
    "y_val = np.concatenate((ART_train_abnormal_labels[abnormal_train_idx:], ART_train_normal_labels[normal_train_idx:]), axis = 0)\n",
    "\n",
    "print(\"Number of samples in train and validation are %d and %d\" %(x_train.shape[0], x_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE\n",
    "\n",
    "PRE_train_abnormal_labels = np.array([1 for _ in range(len(PRE_train_abnormal_scans))])\n",
    "PRE_train_normal_labels = np.array([0 for _ in range(len(PRE_train_normal_scans))])\n",
    "\n",
    "# split 7:3 for validation\n",
    "abnormal_train_idx = round(len(PRE_train_abnormal_labels) * 0.7)\n",
    "normal_train_idx = round(len(PRE_train_normal_labels) * 0.7)\n",
    "\n",
    "x_train = np.concatenate((PRE_train_abnormal_scans[:abnormal_train_idx], PRE_train_normal_scans[:normal_train_idx]), axis = 0)\n",
    "y_train = np.concatenate((PRE_train_abnormal_labels[:abnormal_train_idx], PRE_train_normal_labels[:normal_train_idx]), axis = 0)\n",
    "\n",
    "x_val = np.concatenate((PRE_train_abnormal_scans[abnormal_train_idx:], PRE_train_normal_scans[normal_train_idx:]), axis = 0)\n",
    "y_val = np.concatenate((PRE_train_abnormal_labels[abnormal_train_idx:], PRE_train_normal_labels[normal_train_idx:]), axis = 0)\n",
    "\n",
    "print(\"Number of samples in train and validation are %d and %d\" %(x_train.shape[0], x_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCCT\n",
    "\n",
    "GCCT_train_abnormal_labels = np.array([1 for _ in range(len(GCCT_train_abnormal_scans))])\n",
    "GCCT_train_normal_labels = np.array([0 for _ in range(len(GCCT_train_normal_scans))])\n",
    "\n",
    "# split 7:3 for validation\n",
    "abnormal_train_idx = round(len(GCCT_train_abnormal_labels) * 0.7)\n",
    "normal_train_idx = round(len(GCCT_train_normal_labels) * 0.7)\n",
    "\n",
    "x_train = np.concatenate((GCCT_train_abnormal_scans[:abnormal_train_idx], GCCT_train_normal_scans[:normal_train_idx]), axis = 0)\n",
    "y_train = np.concatenate((GCCT_train_abnormal_labels[:abnormal_train_idx], GCCT_train_normal_labels[:normal_train_idx]), axis = 0)\n",
    "\n",
    "x_val = np.concatenate((GCCT_train_abnormal_scans[abnormal_train_idx:], GCCT_train_normal_scans[normal_train_idx:]), axis = 0)\n",
    "y_val = np.concatenate((GCCT_train_abnormal_labels[abnormal_train_idx:], GCCT_train_normal_labels[normal_train_idx:]), axis = 0)\n",
    "\n",
    "print(\"Number of samples in train and validation are %d and %d\" %(x_train.shape[0], x_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionResNetV2\n",
    "img_size = 512\n",
    "N_CLASSES = 2\n",
    "base_model = InceptionResNetV2(include_top = False,\n",
    "                   weights = 'imagenet',\n",
    "                   input_shape = (img_size,img_size,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = base_model.layers[-2].output\n",
    "last = layers.MaxPool3D(pool_size = 2)(last)\n",
    "last = layers.BatchNormalization()(last)\n",
    "x = layers.GlobalAveragePooling3D()(last)\n",
    "x = Dense(512,'relu')(x)\n",
    "x = Dense(N_CLASSES, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(width = 512, height = 512, depth = 16):  # 기존 16\n",
    "#     inputs = keras.Input((width, height, depth, 1))\n",
    "    \n",
    "# #     x = layers.Conv3D(filters = 32, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "# #     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "# #     x = layers.BatchNormalization()(x)\n",
    "# #     print(x)\n",
    "\n",
    "# #     x = layers.Conv3D(filters = 4, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "# #     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "# #     x = layers.BatchNormalization()(x)\n",
    "# #     print(x)\n",
    "\n",
    "#     x = layers.Conv3D(filters = 8, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "#     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     print(x)\n",
    "\n",
    "#     x = layers.Conv3D(filters = 16, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "#     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     print(x)\n",
    "    \n",
    "#     x = layers.Conv3D(filters = 32, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "#     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     print(x)\n",
    "    \n",
    "#     x = layers.Conv3D(filters = 64, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "#     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     print(x)\n",
    "    \n",
    "#     x = layers.Conv3D(filters = 128, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "#     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     print(x)\n",
    "    \n",
    "# #     x = layers.Conv3D(filters = 256, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "# #     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "# #     x = layers.BatchNormalization()(x)\n",
    "# #     print(x)\n",
    "\n",
    "# #     x = layers.Conv3D(filters = 512, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "# #     x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "# #     x = layers.BatchNormalization()(x)\n",
    "# #     print(x)\n",
    "    \n",
    "#     x = layers.GlobalAveragePooling3D()(x)\n",
    "#     x = layers.Dense(units = 512, activation = \"relu\")(x)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     print(x)\n",
    "    \n",
    "    \n",
    "#     outputs = layers.Dense(units = 1, activation = \"sigmoid\")(x)\n",
    "#     print(outputs)\n",
    "    \n",
    "#     model = keras.Model(inputs, outputs, name = \"3dcnn\")\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(width = 512, height = 512, depth = 16):  # 기존 16\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters = 64, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "\n",
    "    x = layers.Conv3D(filters = 64, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters = 128, kernel_size = 3, activation = \"relu\", padding='same')(inputs)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters = 128, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters = 256, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "\n",
    "    x = layers.Conv3D(filters = 256, kernel_size = 3, activation = \"relu\", padding='same')(x)\n",
    "    x = layers.MaxPool3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    print(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units = 512, activation = \"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    outputs = layers.Dense(units = 1, activation = \"sigmoid\")(x)\n",
    "    print(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name = \"3dcnn\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(width = 512, height = 512, depth = 16) # 기존 16\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps = 100000, decay_rate = 0.96, staircase = True\n",
    ")\n",
    "model.compile(\n",
    "    loss = \"binary_crossentropy\", \n",
    "    optimizer = keras.optimizers.Adam(learning_rate = lr_schedule), \n",
    "    metrics = [\"accuracy\", \"AUC\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART - train\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"./CNN_3D_ART_epoch100_padding8_MassOnly.hdf5\", save_best_only = True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", verbose = 1, patience = 15)\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                            patience =3, \n",
    "#                                            verbose =1,\n",
    "#                                             factor = 0.2,\n",
    "#                                             min_lr =0.0000001)\n",
    "\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data = validation_dataset, \n",
    "        epochs = epochs, \n",
    "        shuffle = True, \n",
    "        verbose = 2, \n",
    "#         callbacks = [checkpoint_cb, early_stopping_cb, learning_rate_reduction]\n",
    "        callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE - train\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"PRE_3d_image_classification-padding16.hdf5\", save_best_only = True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor = \"val_acc\", patience = 15)\n",
    "\n",
    "epochs = 200\n",
    "history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data = validation_dataset, \n",
    "        epochs = epochs, \n",
    "        shuffle = True, \n",
    "        verbose = 2, \n",
    "        callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model performance - ART\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate([\"accuracy\", \"loss\"]): \n",
    "    ax[i].plot(history.history[metric])\n",
    "    ax[i].plot(history.history[\"val_\" + metric])\n",
    "    ax[i].set_title(\"ART Model {}\".format(metric))\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model performance - PRE\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate([\"acc\", \"loss\"]): \n",
    "    ax[i].plot(model.history.history[metric])\n",
    "    ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "    ax[i].set_title(\"PRE Model {}\".format(metric))\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions on a multiple CT scans - ART\n",
    "\n",
    "# n = 0  # random person\n",
    "# load best weights\n",
    "model.load_weights(\"CNN_3D_ART_epoch100_padding8_smallerparameters.hdf5\")\n",
    "\n",
    "for n in range(10): \n",
    "    prediction = model.predict(np.expand_dims(x_val[n], axis = 0))[0]\n",
    "    scores = [1 - prediction[0], prediction[0]]\n",
    "\n",
    "    class_names = [\"normal\", \"abnormal\"]\n",
    "    for score, name in zip(scores, class_names): \n",
    "        print(\n",
    "            \"This model is %.2f percent confident that %dth CT scan is %s\"\n",
    "            % ((100*score), n, name)\n",
    "        )\n",
    "        if (y_val[n] == 1): \n",
    "            true_y = 'abnormal'\n",
    "        else: \n",
    "            true_y = 'normal'\n",
    "    print(\"The true value is: {}\".format(true_y))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on a multiple CT scans - PRE\n",
    "\n",
    "# n = 0  # random person\n",
    "# load best weights\n",
    "model.load_weights(\"PRE_3d_image_classification-padding16.hdf5\")\n",
    "\n",
    "for n in range(10): \n",
    "    prediction = model.predict(np.expand_dims(x_val[n], axis = 0))[0]\n",
    "    scores = [1 - prediction[0], prediction[0]]\n",
    "\n",
    "    class_names = [\"normal\", \"abnormal\"]\n",
    "    for score, name in zip(scores, class_names): \n",
    "        print(\n",
    "            \"This model is %.2f percent confident that %dth CT scan is %s\"\n",
    "            % ((100*score), n, name)\n",
    "        )\n",
    "        if (y_val[n] == 1): \n",
    "            true_y = 'abnormal'\n",
    "        else: \n",
    "            true_y = 'normal'\n",
    "    print(\"The true value is: {}\".format(true_y))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix doesn't work with tensorflow models - solved, but memory error occurs\n",
    "\n",
    "class estimator: \n",
    "    _estimator_type = ''\n",
    "    classes = []\n",
    "    def __init__(self, model, classes): \n",
    "        self.model = model\n",
    "        self._estimator_type = 'classifier'\n",
    "        self.classes_ = classes\n",
    "    def predict(self, X): \n",
    "        y_prob = self.model.predict(X)\n",
    "        y_pred = y_prob.argmax(axis = 1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = estimator(model, label)\n",
    "\n",
    "figsize = (6, 6)\n",
    "label = [\"normal\", \"abnormal\"]\n",
    "plot_confusion_matrix(estimator = classifier, \n",
    "                            X = x_val, \n",
    "                            y_true = y_val, \n",
    "                            display_labels = label, \n",
    "                            cmap = 'Blues', \n",
    "                            normalize = 'true',\n",
    "                            ax = plt.subplots(figsize = figsize)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 3d visualization\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "def plot_basic_object(points): \n",
    "    tri = Delaunay(points).convex_hull\n",
    "    fig = plt.figure(figsize = (8, 8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    S = ax.plot_trisurf(points[:, 0], points[:, 1], points[:2], \n",
    "                       triangles = tri, \n",
    "                       shade = True, cmap = 'Blues', lw = 0.5)\n",
    "    ax.set_xlim3d(-5, 5)\n",
    "    ax.set_ylim3d(-5, 5)\n",
    "    ax.set_zlim3d(-5, 5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_basic_object(x_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
