{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# image data\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 12 08:01:20 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                  Off |\r\n",
      "| N/A   34C    P0    38W / 300W |      0MiB / 32480MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:06.0 Off |                  Off |\r\n",
      "| N/A   34C    P0    64W / 300W |  31117MiB / 32480MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ncp/workspace/blocks3/Gun_code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART_TRAIN N:  2398\n",
      "PRE_TRAIN N:  2369\n"
     ]
    }
   ],
   "source": [
    "# define paths for ART, PRE train datas\n",
    "\n",
    "ART_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/ART/'\n",
    "PRE_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/PRE/'\n",
    "\n",
    "print(\"ART_TRAIN N: \", len(os.listdir(ART_path)))\n",
    "print(\"PRE_TRAIN N: \", len(os.listdir(PRE_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/ART/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART Train : \n",
      "RCC  1091\n",
      "AML + onco  1307\n",
      "----------\n",
      "ART Test : \n",
      "RCC  254\n",
      "AML + onco  69\n",
      "----------\n",
      "PRE Train : \n",
      "RCC  1064\n",
      "AML + onco  1305\n",
      "----------\n",
      "PRE Test : \n",
      "RCC  256\n",
      "AML + onco  75\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# check len of each labels\n",
    "\n",
    "print(\"ART Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"ART Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_test_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_test_path + 'AML + onco/')))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,ResNet101, VGG19, DenseNet201,   MobileNetV2 # ,EfficientNetB4\n",
    "from tensorflow.keras.applications import resnet, vgg16 , vgg19, densenet,  mobilenet_v2 # ,efficientnet\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 512\n",
    "img_width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1919 images belonging to 2 classes.\n",
      "Found 479 images belonging to 2 classes.\n",
      "Found 323 images belonging to 2 classes.\n",
      "Found 1896 images belonging to 2 classes.\n",
      "Found 473 images belonging to 2 classes.\n",
      "Found 331 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "\n",
    "image_shape = (512, 512, 3)\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                       preprocessing_function=resnet.preprocess_input,\n",
    "                                       validation_split = 0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                  preprocessing_function=resnet.preprocess_input, \n",
    "                                  rescale= 1./255.)\n",
    "\n",
    "ART_train_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "ART_val_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "ART_test_generator = test_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "                                                    \n",
    "\n",
    "PRE_train_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "PRE_val_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "PRE_test_generator = test_datagen.flow_from_directory(PRE_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "# for layer in res_model.layers:\n",
    "#     if 'conv5' not in layer.name:\n",
    "#         layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = ResNet50(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res_model.layers:\n",
    "    if 'conv5' not in layer.name:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(res_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART train\n",
    "checkpointer = ModelCheckpoint(filepath='./mass_ART-ResNet50.hdf5',\n",
    "                           monitor='val_loss', verbose = 1,\n",
    "                           save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping,learning_rate_reduction])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE train\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_PRE-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(PRE_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 50,\n",
    "                    verbose = 1,\n",
    "                    validation_data = PRE_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_ART = '/home/ncp/workspace/blocks3/zio_code/cancer_ART-ResNet50.hdf5'\n",
    "ART_resnet = tf.keras.models.load_model(path_resnet_ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_PRE = '/home/ncp/workspace/blocks3/zio_code/cancer_PRE-ResNet50.hdf5'\n",
    "PRE_resnet = tf.keras.models.load_model(path_resnet_PRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create label - with configuration image           \n",
    "run '2. Create label' section first to create label dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 kidney_data - ART, PRE - Test, Train imageset 을 3개의 label별 폴더로 분류\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Train_data/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Train_data/PRE/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Test_data/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Test_data/PRE/'\n",
    "\n",
    "new_ART_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/ART/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/PRE/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/PRE/'\n",
    "\n",
    "# make dir\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/'\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "\n",
    "# create img name list\n",
    "art_train_list = os.listdir(ART_train_path)\n",
    "pre_train_list = os.listdir(PRE_train_path)\n",
    "art_test_list = os.listdir(ART_test_path)\n",
    "pre_test_list = os.listdir(PRE_test_path)\n",
    "\n",
    "\n",
    "# ART train\n",
    "for img in tqdm(art_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = ART_train_label.loc[ART_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(ART_train_path + img, new_ART_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(ART_train_path + img, new_ART_train_path + 'AML + onco/' + img)\n",
    "\n",
    "# PRE train\n",
    "for img in tqdm(pre_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(PRE_train_path + img, new_PRE_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(PRE_train_path + img, new_PRE_train_path + 'AML + onco/' + img)\n",
    "        \n",
    "# ART test\n",
    "for img in tqdm(art_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = ART_val_label.loc[ART_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(ART_test_path + img, new_ART_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(ART_test_path + img, new_ART_test_path + 'AML + onco/' + img)\n",
    "        \n",
    "# PRE test\n",
    "for img in tqdm(pre_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(PRE_test_path + img, new_PRE_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(PRE_test_path + img, new_PRE_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check len of each labels\n",
    "\n",
    "print(\"ART Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"ART Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_test_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_test_path + 'AML + onco/')))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification - with configuration image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,ResNet101, VGG19, DenseNet201,   MobileNetV2 # ,EfficientNetB4\n",
    "from tensorflow.keras.applications import resnet, vgg16 , vgg19, densenet,  mobilenet_v2 # ,efficientnet\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus =tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    # tensorflow가 첫번째 gpu만 사용하도록 제한\n",
    "    try: \n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.is_gpu_available()\n",
    "#tf.test.is_built_with_cuda()\n",
    "tf.test.is_built_with_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 512\n",
    "img_width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/PRE/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/PRE/'\n",
    "\n",
    "image_shape = (512, 512, 3)\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                       rescale= 1./255.,\n",
    "                                       validation_split = 0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                       rescale= 1./255.)\n",
    "\n",
    "ART_train_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "ART_val_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "ART_test_generator = test_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "                                                    \n",
    "\n",
    "PRE_train_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "PRE_val_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "PRE_test_generator = test_datagen.flow_from_directory(PRE_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "for layer in res_model.layers:\n",
    "    if 'conv5' not in layer.name:\n",
    "        layer.trainable = False\n",
    "# Check if all layers except conv5 layers are not trainable\n",
    "#for i, layer in enumerate(res_model.layers):\n",
    "#    print(i, layer.name, \"-\", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "model = Sequential()\n",
    "model.add(res_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART train\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_ART-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.isgpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_res.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_res.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = tf.keras.models.load_model('./cancer_configuration_ART-ResNet50.hdf5')\n",
    "model_test.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE train\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_PRE-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(PRE_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = PRE_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(300))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ART Train\n",
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_ART-ResNet101.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res101 = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res101.history['acc'], label = 'train',)\n",
    "plt.plot(history_res101.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE Train\n",
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_PRE-ResNet101.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res101 = model.fit(PRE_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = PRE_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res101.history['acc'], label = 'train',)\n",
    "plt.plot(history_res101.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation         \n",
    "change image data directory format to perform cross validation        \n",
    "모델은 ResNet101 을 사용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of file list\n",
    "# 라벨별로 분리해둔 폴더에서 filename - label 쌍의 dataframe을 생성\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "#GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "#GCCT_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "\n",
    "# ART_train filename + label DF\n",
    "ART_train_RCC = pd.DataFrame(os.listdir(ART_train_path + 'RCC/'))\n",
    "ART_train_RCC['label'] = 'RCC'\n",
    "ART_train_RCC.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "ART_train_AMLonco = pd.DataFrame(os.listdir(ART_train_path + 'AML + onco/'))\n",
    "ART_train_AMLonco['label'] = 'AML + onco'\n",
    "ART_train_AMLonco.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "ART_train_df = pd.concat([ART_train_RCC, ART_train_AMLonco], axis = 0)\n",
    "ART_train_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# PRE_train filename + label DF\n",
    "PRE_train_RCC = pd.DataFrame(os.listdir(PRE_train_path + 'RCC/'))\n",
    "PRE_train_RCC['label'] = 'RCC'\n",
    "PRE_train_RCC.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "PRE_train_AMLonco = pd.DataFrame(os.listdir(PRE_train_path + 'AML + onco/'))\n",
    "PRE_train_AMLonco['label'] = 'AML + onco'\n",
    "PRE_train_AMLonco.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "PRE_train_df = pd.concat([PRE_train_RCC, PRE_train_AMLonco], axis = 0)\n",
    "PRE_train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GCCT \n",
    "\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "new_GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "new_GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/'\n",
    "makedir(dir)\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "# create img name list\n",
    "GCCT_train_list = os.listdir(GCCT_train_path)\n",
    "GCCT_test_list = os.listdir(GCCT_test_path)\n",
    "\n",
    "# GCCT train / we can use same label for GCCT and PRE \n",
    "for img in tqdm(GCCT_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(GCCT_train_path + img, new_GCCT_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(GCCT_train_path + img, new_GCCT_train_path + 'AML + onco/' + img)\n",
    "        \n",
    "        \n",
    "# GCCT test\n",
    "for img in tqdm(GCCT_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(GCCT_test_path + img, new_GCCT_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(GCCT_test_path + img, new_GCCT_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_shape = (512, 512, 3)\n",
    "# res101_model = ResNet101(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "# for layer in res101_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet101 (Model)            (None, 2048)              42658176  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 42,670,466\n",
      "Trainable params: 42,561,026\n",
      "Non-trainable params: 109,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 - ResNet101\n",
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dense(300))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(100))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "\n",
    "# ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# # create datagen\n",
    "# create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# # generate test data first\n",
    "# ART_test_generator = create_datagen.flow_from_directory(ART_test_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323 images belonging to 2 classes.\n",
      "Found 1918 validated image filenames belonging to 2 classes.\n",
      "Found 480 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9812\n",
      "Epoch 00001: val_loss improved from inf to 0.01589, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.0159 - val_accuracy: 0.9937 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9734\n",
      "Epoch 00002: val_loss improved from 0.01589 to 0.01395, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 733ms/step - loss: 0.0830 - accuracy: 0.9734 - val_loss: 0.0140 - val_accuracy: 0.9937 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9875\n",
      "Epoch 00003: val_loss did not improve from 0.01395\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 0.0531 - accuracy: 0.9875 - val_loss: 0.0311 - val_accuracy: 0.9833 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9922\n",
      "Epoch 00004: val_loss improved from 0.01395 to 0.01051, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 719ms/step - loss: 0.0341 - accuracy: 0.9922 - val_loss: 0.0105 - val_accuracy: 0.9979 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9781\n",
      "Epoch 00005: val_loss improved from 0.01051 to 0.00899, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 729ms/step - loss: 0.0586 - accuracy: 0.9781 - val_loss: 0.0090 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9937\n",
      "Epoch 00006: val_loss did not improve from 0.00899\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0151 - accuracy: 0.9937 - val_loss: 0.0930 - val_accuracy: 0.9563 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.00899\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 0.9958 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9937\n",
      "Epoch 00008: val_loss did not improve from 0.00899\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0140 - accuracy: 0.9937 - val_loss: 0.0281 - val_accuracy: 0.9937 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9984\n",
      "Epoch 00009: val_loss improved from 0.00899 to 0.00603, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0060 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss improved from 0.00603 to 0.00418, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9969\n",
      "Epoch 00011: val_loss improved from 0.00418 to 0.00338, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 13s 668ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0034 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0893e-04 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00338\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 8.0893e-04 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8894e-04 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 6.8894e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00338\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 00019: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.0065 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00338\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000  \n",
      "Epoch 00021: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5308e-04 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.00338\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 9.5308e-04 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9138e-04 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 6.9138e-04 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00338\n",
      "20/20 [==============================] - 10s 510ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
      "Epoch 00026: val_loss did not improve from 0.00338\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 00026: early stopping\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Found 1918 validated image filenames belonging to 2 classes.\n",
      "Found 480 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9922\n",
      "Epoch 00001: val_loss improved from inf to 0.05815, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 0.0494 - accuracy: 0.9922 - val_loss: 0.0581 - val_accuracy: 0.9812 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9812\n",
      "Epoch 00002: val_loss did not improve from 0.05815\n",
      "20/20 [==============================] - 10s 522ms/step - loss: 0.0391 - accuracy: 0.9812 - val_loss: 0.1618 - val_accuracy: 0.9417 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9875\n",
      "Epoch 00003: val_loss did not improve from 0.05815\n",
      "20/20 [==============================] - 10s 512ms/step - loss: 0.0340 - accuracy: 0.9875 - val_loss: 0.1331 - val_accuracy: 0.9417 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9781\n",
      "Epoch 00004: val_loss did not improve from 0.05815\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 0.0574 - accuracy: 0.9781 - val_loss: 0.1189 - val_accuracy: 0.9625 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9906\n",
      "Epoch 00005: val_loss did not improve from 0.05815\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0392 - accuracy: 0.9906 - val_loss: 0.0886 - val_accuracy: 0.9688 - lr: 2.0000e-05\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9922\n",
      "Epoch 00006: val_loss improved from 0.05815 to 0.03536, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 723ms/step - loss: 0.0211 - accuracy: 0.9922 - val_loss: 0.0354 - val_accuracy: 0.9875 - lr: 2.0000e-05\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9984\n",
      "Epoch 00007: val_loss improved from 0.03536 to 0.02178, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 723ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.0218 - val_accuracy: 0.9917 - lr: 2.0000e-05\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss improved from 0.02178 to 0.00836, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 682ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9969\n",
      "Epoch 00009: val_loss improved from 0.00836 to 0.00699, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.0070 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9984\n",
      "Epoch 00010: val_loss did not improve from 0.00699\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0079 - val_accuracy: 0.9958 - lr: 2.0000e-05\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9984\n",
      "Epoch 00011: val_loss improved from 0.00699 to 0.00583, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 723ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0058 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9984\n",
      "Epoch 00012: val_loss improved from 0.00583 to 0.00523, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 726ms/step - loss: 0.0025 - accuracy: 0.9984 - val_loss: 0.0052 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss improved from 0.00523 to 0.00483, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 724ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9958 - lr: 2.0000e-05\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4975e-04 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss improved from 0.00483 to 0.00458, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 681ms/step - loss: 5.4975e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9958 - lr: 2.0000e-05\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9953\n",
      "Epoch 00015: val_loss improved from 0.00458 to 0.00329, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 722ms/step - loss: 0.0092 - accuracy: 0.9953 - val_loss: 0.0033 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss improved from 0.00329 to 0.00236, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 732ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss improved from 0.00236 to 0.00228, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 13s 671ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9984\n",
      "Epoch 00018: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0024 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9984\n",
      "Epoch 00019: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0025 - accuracy: 0.9984 - val_loss: 0.0026 - val_accuracy: 0.9979 - lr: 2.0000e-05\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7521e-04 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 523ms/step - loss: 6.7521e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "20/20 [==============================] - 10s 513ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9984\n",
      "Epoch 00023: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 513ms/step - loss: 0.0089 - accuracy: 0.9984 - val_loss: 0.0028 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2051e-04 - accuracy: 1.0000\n",
      "Epoch 00024: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 8.2051e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9984\n",
      "Epoch 00025: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0041 - accuracy: 0.9984 - val_loss: 0.0029 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7267e-04 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 7.7267e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4903e-04 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 5.4903e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9754e-04 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 5.9754e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.8766e-04 - accuracy: 1.0000\n",
      "Epoch 00031: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 2.8766e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9974e-04 - accuracy: 1.0000\n",
      "Epoch 00032: val_loss did not improve from 0.00228\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 4.9974e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 00032: early stopping\n",
      "15/15 [==============================] - 4s 271ms/step - loss: 0.0032 - accuracy: 0.9979\n",
      "Found 1918 validated image filenames belonging to 2 classes.\n",
      "Found 480 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9937\n",
      "Epoch 00001: val_loss improved from inf to 0.01749, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 16s 816ms/step - loss: 0.0143 - accuracy: 0.9937 - val_loss: 0.0175 - val_accuracy: 0.9917 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9906\n",
      "Epoch 00002: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0183 - accuracy: 0.9906 - val_loss: 0.0523 - val_accuracy: 0.9854 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9875\n",
      "Epoch 00003: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0463 - accuracy: 0.9875 - val_loss: 0.2693 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9844\n",
      "Epoch 00004: val_loss did not improve from 0.01749\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0356 - accuracy: 0.9844 - val_loss: 0.7838 - val_accuracy: 0.7854 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9922\n",
      "Epoch 00005: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0216 - accuracy: 0.9922 - val_loss: 0.3335 - val_accuracy: 0.8792 - lr: 2.0000e-05\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9953\n",
      "Epoch 00006: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0251 - accuracy: 0.9953 - val_loss: 0.1077 - val_accuracy: 0.9708 - lr: 2.0000e-05\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9953\n",
      "Epoch 00007: val_loss did not improve from 0.01749\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0078 - accuracy: 0.9953 - val_loss: 0.0288 - val_accuracy: 0.9958 - lr: 2.0000e-05\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9984\n",
      "Epoch 00008: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 522ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0237 - val_accuracy: 0.9958 - lr: 4.0000e-06\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9969\n",
      "Epoch 00009: val_loss did not improve from 0.01749\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.0209 - val_accuracy: 0.9958 - lr: 4.0000e-06\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9937\n",
      "Epoch 00010: val_loss improved from 0.01749 to 0.01672, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 730ms/step - loss: 0.0123 - accuracy: 0.9937 - val_loss: 0.0167 - val_accuracy: 0.9958 - lr: 4.0000e-06\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9984\n",
      "Epoch 00011: val_loss improved from 0.01672 to 0.01348, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 729ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0135 - val_accuracy: 0.9958 - lr: 4.0000e-06\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9937\n",
      "Epoch 00012: val_loss improved from 0.01348 to 0.01209, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 681ms/step - loss: 0.0137 - accuracy: 0.9937 - val_loss: 0.0121 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9984\n",
      "Epoch 00013: val_loss improved from 0.01209 to 0.01121, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 729ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 0.0112 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9984\n",
      "Epoch 00014: val_loss improved from 0.01121 to 0.01049, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 730ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0105 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9984\n",
      "Epoch 00015: val_loss did not improve from 0.01049\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0037 - accuracy: 0.9984 - val_loss: 0.0106 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9984\n",
      "Epoch 00016: val_loss did not improve from 0.01049\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 0.0035 - accuracy: 0.9984 - val_loss: 0.0107 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9969\n",
      "Epoch 00017: val_loss did not improve from 0.01049\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0058 - accuracy: 0.9969 - val_loss: 0.0106 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 0.01049\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 00019: val_loss improved from 0.01049 to 0.01010, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 725ms/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.0101 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss improved from 0.01010 to 0.00997, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 724ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss improved from 0.00997 to 0.00982, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 683ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9984\n",
      "Epoch 00022: val_loss improved from 0.00982 to 0.00925, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 720ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 0.0092 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2700e-04 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss improved from 0.00925 to 0.00917, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 722ms/step - loss: 7.2700e-04 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9984\n",
      "Epoch 00024: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 0.0098 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 0.00917\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 0.9979 - lr: 8.0000e-07\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9505e-04 - accuracy: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 6.9505e-04 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9937\n",
      "Epoch 00027: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0230 - accuracy: 0.9937 - val_loss: 0.0098 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.00917\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 11s 529ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 0.9979 - lr: 1.6000e-07\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8290e-04 - accuracy: 1.0000\n",
      "Epoch 00029: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 7.8290e-04 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8967e-04 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 7.8967e-04 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9953\n",
      "Epoch 00031: val_loss did not improve from 0.00917\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0074 - accuracy: 0.9953 - val_loss: 0.0096 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1290e-04 - accuracy: 1.0000\n",
      "Epoch 00032: val_loss did not improve from 0.00917\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 8.1290e-04 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00033: val_loss improved from 0.00917 to 0.00896, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 723ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00034: val_loss improved from 0.00896 to 0.00859, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 722ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4119e-04 - accuracy: 1.0000\n",
      "Epoch 00035: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 5.4119e-04 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00036: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00037: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5542e-04 - accuracy: 1.0000\n",
      "Epoch 00038: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 6.5542e-04 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9979 - lr: 1.0000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00039: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 521ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00040: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00041: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 512ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9984\n",
      "Epoch 00042: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0089 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00043: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 513ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00044: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00045: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9984\n",
      "Epoch 00046: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0091 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00047: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00048: val_loss did not improve from 0.00859\n",
      "20/20 [==============================] - 10s 514ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9969\n",
      "Epoch 00049: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0055 - accuracy: 0.9969 - val_loss: 0.0094 - val_accuracy: 0.9979 - lr: 1.0000e-07\n",
      "Epoch 00049: early stopping\n",
      "15/15 [==============================] - 4s 275ms/step - loss: 0.0094 - accuracy: 0.9979\n",
      "Found 1919 validated image filenames belonging to 2 classes.\n",
      "Found 479 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9937\n",
      "Epoch 00001: val_loss improved from inf to 0.01678, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 17s 830ms/step - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.0168 - val_accuracy: 0.9979 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9922\n",
      "Epoch 00002: val_loss did not improve from 0.01678\n",
      "20/20 [==============================] - 10s 519ms/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.0278 - val_accuracy: 0.9937 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9859\n",
      "Epoch 00003: val_loss did not improve from 0.01678\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0331 - accuracy: 0.9859 - val_loss: 0.0774 - val_accuracy: 0.9833 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9687\n",
      "Epoch 00004: val_loss did not improve from 0.01678\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "20/20 [==============================] - 10s 520ms/step - loss: 0.0915 - accuracy: 0.9687 - val_loss: 0.0653 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9922\n",
      "Epoch 00005: val_loss did not improve from 0.01678\n",
      "20/20 [==============================] - 10s 516ms/step - loss: 0.0175 - accuracy: 0.9922 - val_loss: 0.0415 - val_accuracy: 0.9833 - lr: 2.0000e-05\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9937\n",
      "Epoch 00006: val_loss did not improve from 0.01678\n",
      "20/20 [==============================] - 10s 515ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.0243 - val_accuracy: 0.9896 - lr: 2.0000e-05\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9969\n",
      "Epoch 00007: val_loss improved from 0.01678 to 0.01440, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 728ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.0144 - val_accuracy: 0.9937 - lr: 2.0000e-05\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9984\n",
      "Epoch 00008: val_loss improved from 0.01440 to 0.00601, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 731ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0060 - val_accuracy: 0.9958 - lr: 2.0000e-05\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9969\n",
      "Epoch 00009: val_loss improved from 0.00601 to 0.00398, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 14s 678ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0040 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9969\n",
      "Epoch 00010: val_loss improved from 0.00398 to 0.00170, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 732ms/step - loss: 0.0062 - accuracy: 0.9969 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9984\n",
      "Epoch 00011: val_loss improved from 0.00170 to 0.00101, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 735ms/step - loss: 0.0028 - accuracy: 0.9984 - val_loss: 0.0010 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss improved from 0.00101 to 0.00086, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 733ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.6051e-04 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss improved from 0.00086 to 0.00065, saving model to ./2D_classification_ART-ResNet101_crossval.hdf5\n",
      "20/20 [==============================] - 15s 732ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.4715e-04 - val_accuracy: 1.0000 - lr: 2.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 0.00065\n",
      "20/20 [==============================] - 10s 517ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 7.0196e-04 - val_accuracy: 1.0000 - lr: 2.0000e-05\n",
      "Epoch 15/100\n",
      "15/20 [=====================>........] - ETA: 1s - loss: 0.0020 - accuracy: 0.9979"
     ]
    }
   ],
   "source": [
    "# ART\n",
    "# 다시 라벨별 분리하지 않은 데이터 경로에서 작업 (덕선선생님이 만드신것)\n",
    "# test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "# 1007 learning rate reduction 추가\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/PRE/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "ART_test_generator = create_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(ART_train_df, ART_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = ART_train_df.iloc[train_index]\n",
    "    validation_data = ART_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./2D_classification_ART-ResNet101_crossval.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        steps_per_epoch = 20,\n",
    "                        epochs = 100,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_resnet_ART = '/home/ncp/workspace/blocks3/zio_code/cancer_configuration_ART-ResNet101_crossval.hdf5'\n",
    "# ART_resnet = tf.keras.models.load_model(path_resnet_ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ART =model\n",
    "result = model_ART.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_res101.history['accuracy'], label = 'train',)\n",
    "plt.plot(history_res101.history['val_accuracy'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dense(300))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(100))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GCCT\n",
    "# 다시 라벨별 분리하지 않은 데이터 경로에서 작업 (덕선선생님이 만드신것)\n",
    "# test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "# 1007 learning rate reduction 추가\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "GCCT_test_generator = create_datagen.flow_from_directory(GCCT_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(PRE_train_df, PRE_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = PRE_train_df.iloc[train_index]\n",
    "    validation_data = PRE_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./2D_classification_ART-ResNet101_crossval.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        steps_per_epoch = 20,\n",
    "                        epochs = 100,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_GCCT = model\n",
    "\n",
    "result = model_GCCT.evaluate(GCCT_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(history_res101.history['accuracy'], label = 'train',)\n",
    "plt.plot(history_res101.history['val_accuracy'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_PRE = '/home/ncp/workspace/blocks3/zio_code/cancer_configuration_PRE-ResNet101_crossval.hdf5'\n",
    "PRE_resnet = tf.keras.models.load_model(path_resnet_PRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dense(300))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(100))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE\n",
    "# 다시 라벨별 분리하지 않은 데이터 경로에서 작업 (덕선선생님이 만드신것)\n",
    "# test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "# 1007 learning rate reduction 추가\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/PRE/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "PRE_test_generator = create_datagen.flow_from_directory(PRE_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY_PRE = []\n",
    "VALIDATION_LOSS_PRE = []\n",
    "val_acc_PRE = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(PRE_train_df, PRE_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = PRE_train_df.iloc[train_index]\n",
    "    validation_data = PRE_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = PRE_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = PRE_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./2D_classification_PRE-ResNet101_crossval.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101_PRE = model.fit(train_data_generator,\n",
    "                        steps_per_epoch = 20,\n",
    "                        epochs = 100,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results_PRE = model.evaluate(valid_data_generator)\n",
    "    results_PRE = dict(zip(model.metrics_names, results_PRE))\n",
    "    \n",
    "    VALIDATION_ACCURACY_PRE.append(results_PRE['accuracy'])\n",
    "    VALIDATION_LOSS_PRE.append(results_PRE['loss'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc_PRE += results_PRE['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc_PRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre = model \n",
    "result = model_pre.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.05856511369347572],\n",
       " 'accuracy': [0.9825119376182556],\n",
       " 'val_loss': [0.003313901135697961],\n",
       " 'val_accuracy': [1.0]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_res101_PRE.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_res101_PRE.history['accuracy'], label = 'train',)\n",
    "plt.plot(history_res101_PRE.history['val_accuracy'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_PRE = '/home/ncp/workspace/blocks3/zio_code/cancer_configuration_PRE-ResNet101_crossval.hdf5'\n",
    "PRE_resnet = tf.keras.models.load_model(path_resnet_PRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_PRE = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res101_PRE.history['accuracy'], label = 'train',)\n",
    "plt.plot(history_res101_PRE.history['val_accuracy'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddd",
   "language": "python",
   "name": "ddd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
