{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# image data\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for ART, PRE train datas\n",
    "\n",
    "ART_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly/ART/'\n",
    "PRE_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly/PRE/'\n",
    "\n",
    "print(\"ART N: \", len(os.listdir(ART_path)))\n",
    "print(\"PRE N: \", len(os.listdir(PRE_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm header check\n",
    "import pydicom\n",
    "\n",
    "path__ = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/1.Training/원천데이터/02_01_0010/02_01_0010_ART/0001.dcm'\n",
    "header_01 = pydicom.dcmread(path__, stop_before_pixels = True)\n",
    "header_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ART_train_label dataframe\n",
    "\n",
    "LABEL_path = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/1.Training/라벨링데이터/'\n",
    "people = os.listdir(LABEL_path) \n",
    "ART_train_label = pd.DataFrame()\n",
    "\n",
    "for ppl in tqdm(people): \n",
    "    ppl_ARTlabel_path = LABEL_path + ppl + '/' + '{}_ART.json'.format(ppl)\n",
    "    with open(ppl_ARTlabel_path) as f: \n",
    "        ART_json = json.load(f)\n",
    "    case_id = ART_json['case_id'][0:10]\n",
    "    type = ART_json['Clinical Information (Global)']['histologic type']\n",
    "    data = {'case_id' : [case_id], 'histologic type' : [type]}\n",
    "    data = pd.DataFrame(data)\n",
    "    ART_train_label = pd.concat([ART_train_label, data]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ART_val_label dataframe\n",
    "\n",
    "LABEL_path = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/2.Validation/라벨링데이터/'\n",
    "people = os.listdir(LABEL_path) \n",
    "ART_val_label = pd.DataFrame()\n",
    "\n",
    "for ppl in tqdm(people): \n",
    "    ppl_ARTlabel_path = LABEL_path + ppl + '/' + '{}_ART.json'.format(ppl)\n",
    "    with open(ppl_ARTlabel_path) as f: \n",
    "        ART_json = json.load(f)\n",
    "    case_id = ART_json['case_id'][0:10]\n",
    "    type = ART_json['Clinical Information (Global)']['histologic type']\n",
    "    data = {'case_id' : [case_id], 'histologic type' : [type]}\n",
    "    data = pd.DataFrame(data)\n",
    "    ART_val_label = pd.concat([ART_val_label, data]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PRE_train_label dataframe\n",
    "\n",
    "LABEL_path = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/1.Training/라벨링데이터/'\n",
    "people = os.listdir(LABEL_path) \n",
    "PRE_train_label = pd.DataFrame()\n",
    "\n",
    "for ppl in tqdm(people): \n",
    "    ppl_PRElabel_path = LABEL_path + ppl + '/' + '{}_PRE.json'.format(ppl)\n",
    "    with open(ppl_PRElabel_path) as f: \n",
    "        PRE_json = json.load(f)\n",
    "    case_id = PRE_json['case_id'][0:10]\n",
    "    type = PRE_json['Clinical Information (Global)']['histologic type']\n",
    "    data = {'case_id' : [case_id], 'histologic type' : [type]}\n",
    "    data = pd.DataFrame(data)\n",
    "    PRE_train_label = pd.concat([PRE_train_label, data]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PRE_val_label dataframe\n",
    "\n",
    "LABEL_path = '/home/ncp/workspace/202002n035/035.신장암 진단을 위한 의료 영상 데이터/01.데이터/신장암2/2.Validation/라벨링데이터/'\n",
    "people = os.listdir(LABEL_path) \n",
    "PRE_val_label = pd.DataFrame()\n",
    "\n",
    "for ppl in tqdm(people): \n",
    "    ppl_PRElabel_path = LABEL_path + ppl + '/' + '{}_PRE.json'.format(ppl)\n",
    "    with open(ppl_PRElabel_path) as f: \n",
    "        PRE_json = json.load(f)\n",
    "    case_id = PRE_json['case_id'][0:10]\n",
    "    type = PRE_json['Clinical Information (Global)']['histologic type']\n",
    "    data = {'case_id' : [case_id], 'histologic type' : [type]}\n",
    "    data = pd.DataFrame(data)\n",
    "    PRE_val_label = pd.concat([PRE_val_label, data]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_train_label['histologic type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new label in label_df\n",
    "    # cc, chr, pp -> RCC\n",
    "    # AML\n",
    "    # ONCO\n",
    "# (07.28 수정 반영) RCC // AML + onco 로 binary classification\n",
    "\n",
    "def new_label(df): \n",
    "    df['label'] = df['histologic type']\n",
    "    df.loc[df['label'].isin(['cc', 'chr', 'pp']) == True, 'label'] = 'RCC'\n",
    "    df.loc[df['label'] == 'AML', 'label'] = 'AML + onco'\n",
    "    df.loc[df['label'] == 'onco', 'label'] = 'AML + onco'\n",
    "    return df\n",
    "\n",
    "ART_train_label = new_label(ART_train_label)\n",
    "PRE_train_label = new_label(PRE_train_label)\n",
    "ART_val_label = new_label(ART_val_label)\n",
    "PRE_val_label = new_label(PRE_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "\n",
    "print(\"ART_train_label count: \")\n",
    "print(ART_train_label['label'].value_counts())\n",
    "print(\"PRE_train_label count: \")\n",
    "print(PRE_train_label['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "\n",
    "print(\"ART_val_label count: \")\n",
    "print(ART_val_label['label'].value_counts())\n",
    "print(\"PRE_val_label count: \")\n",
    "print(PRE_val_label['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kidneyData_windowing_mass_classified 폴더는 만들고 돌려야 함\n",
    "shutil.rmtree('/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TRAIN/')\n",
    "shutil.rmtree('/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TEST/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 kidney_data - ART, PRE - Test, Train imageset 을 2개의 label별 폴더로 분류 (TRAIN/TEST 분리까지 포함)\n",
    "\n",
    "ART_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/ART/'\n",
    "PRE_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/PRE/'\n",
    "GCCT_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/GCCT/'\n",
    "\n",
    "new_ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TRAIN/ART/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "new_GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TRAIN/GCCT/'\n",
    "new_GCCT_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/TEST/GCCT/'\n",
    "\n",
    "# make dir\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_classified/'\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "\n",
    "# create img name list\n",
    "art_list = os.listdir(ART_path)\n",
    "pre_list = os.listdir(PRE_path)\n",
    "gcct_list = os.listdir(GCCT_path)\n",
    "\n",
    "\n",
    "# ART\n",
    "for img in tqdm(art_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        label = ART_train_label.loc[ART_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(ART_path + img, new_ART_train_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(ART_path + img, new_ART_train_path + 'AML + onco/' + img)\n",
    "    else: # test\n",
    "        label = ART_val_label.loc[ART_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(ART_path + img, new_ART_test_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(ART_path + img, new_ART_test_path + 'AML + onco/' + img)\n",
    "\n",
    "# PRE\n",
    "for img in tqdm(pre_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_train_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_train_path + 'AML + onco/' + img)\n",
    "    else: # test\n",
    "        label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_test_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_test_path + 'AML + onco/' + img)\n",
    "        \n",
    "# GCCT\n",
    "for img in tqdm(gcct_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(GCCT_path + img, new_GCCT_train_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(GCCT_path + img, new_GCCT_train_path + 'AML + onco/' + img)\n",
    "    else: # test\n",
    "        label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(GCCT_path + img, new_GCCT_test_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(GCCT_path + img, new_GCCT_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check len of each labels - Train/Test 분리되었을 시\n",
    "\n",
    "print(\"ART Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"ART Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_test_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_test_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"GCCT Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_GCCT_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_GCCT_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"GCCT Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_GCCT_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_GCCT_test_path + 'AML + onco/')))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from keras.utils import np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,ResNet101, VGG19, DenseNet201,   MobileNetV2 # ,EfficientNetB4\n",
    "from tensorflow.keras.applications import resnet, vgg16 , vgg19, densenet,  mobilenet_v2 # ,efficientnet\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus =tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    # tensorflow가 첫번째 gpu만 사용하도록 제한\n",
    "    try: \n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.is_gpu_available()\n",
    "#tf.test.is_built_with_cuda()\n",
    "tf.test.is_built_with_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 512\n",
    "img_width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/ART/'\n",
    "# ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "# PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "# PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "# # GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/GCCT/'\n",
    "# # GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/GCCT/'\n",
    "\n",
    "# image_shape = (512, 512, 3)\n",
    "# N_CLASSES = 2\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# train_datagen = ImageDataGenerator(dtype='float32', \n",
    "#                                        preprocessing_function=resnet.preprocess_input,\n",
    "#                                        validation_split = 0.2)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(dtype='float32', \n",
    "#                                   preprocessing_function=resnet.preprocess_input, \n",
    "#                                   rescale= 1./255.)\n",
    "\n",
    "# ART_train_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'training')\n",
    "# ART_val_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'validation')\n",
    "# ART_test_generator = test_datagen.flow_from_directory(ART_test_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical')\n",
    "                                                    \n",
    "\n",
    "# PRE_train_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'training')\n",
    "# PRE_val_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'validation')\n",
    "# PRE_test_generator = test_datagen.flow_from_directory(PRE_test_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical')\n",
    "\n",
    "# GCCT_train_generator = train_datagen.flow_from_directory(GCCT_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'training')\n",
    "# GCCT_val_generator = train_datagen.flow_from_directory(GCCT_train_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical', \n",
    "#                                                     subset = 'validation')\n",
    "# GCCT_test_generator = test_datagen.flow_from_directory(GCCT_test_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 그냥 convolution block 쌓아서 해보자\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = image_shape))\n",
    "# model.add(layers.MaxPooling2D((2,2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# model.add(layers.MaxPooling2D((2,2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation = 'relu'))\n",
    "# model.add(layers.Dense(N_CLASSES, activation = 'softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "# model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ART train\n",
    "# checkpointer = ModelCheckpoint(filepath='./mass_ART.hdf5',\n",
    "#                             monitor='val_loss', verbose = 1,\n",
    "#                             save_best_only=True)\n",
    "# early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "# history_res = model.fit(ART_train_generator,\n",
    "#                     steps_per_epoch = 20,\n",
    "#                     epochs = 100,\n",
    "#                     verbose = 1,\n",
    "#                     validation_data = ART_val_generator,\n",
    "#                     callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "# for layer in res_model.layers:\n",
    "#     if 'conv5' not in layer.name:\n",
    "#         layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = ResNet50(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res_model.layers:\n",
    "    if 'conv5' not in layer.name:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(res_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART train\n",
    "checkpointer = ModelCheckpoint(filepath='./mass_ART-ResNet50.hdf5',\n",
    "                           monitor='val_loss', verbose = 1,\n",
    "                           save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping,learning_rate_reduction])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE train\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_PRE-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(PRE_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 50,\n",
    "                    verbose = 1,\n",
    "                    validation_data = PRE_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_ART = '/home/ncp/workspace/blocks3/zio_code/cancer_ART-ResNet50.hdf5'\n",
    "ART_resnet = tf.keras.models.load_model(path_resnet_ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_PRE = '/home/ncp/workspace/blocks3/zio_code/cancer_PRE-ResNet50.hdf5'\n",
    "PRE_resnet = tf.keras.models.load_model(path_resnet_PRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create label - with configuration image           \n",
    "run '2. Create label' section first to create label dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GCCT \n",
    "\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "new_GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/TRAIN/GCCT/'\n",
    "new_GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/TEST/GCCT/'\n",
    "\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass/'\n",
    "makedir(dir)\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "# create img name list\n",
    "GCCT_train_list = os.listdir(GCCT_train_path)\n",
    "GCCT_test_list = os.listdir(GCCT_test_path)\n",
    "\n",
    "# GCCT train / we can use same label for GCCT and PRE \n",
    "for img in tqdm(GCCT_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(GCCT_train_path + img, new_GCCT_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(GCCT_train_path + img, new_GCCT_train_path + 'AML + onco/' + img)\n",
    "        \n",
    "        \n",
    "# GCCT test\n",
    "for img in tqdm(GCCT_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(GCCT_test_path + img, new_GCCT_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(GCCT_test_path + img, new_GCCT_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 kidney_data - ART, PRE - Test, Train imageset 을 3개의 label별 폴더로 분류\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Train_data/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Train_data/PRE/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Test_data/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing/Test_data/PRE/'\n",
    "\n",
    "new_ART_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/ART/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TRAIN/PRE/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/PRE/'\n",
    "\n",
    "# make dir\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/'\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "\n",
    "# create img name list\n",
    "art_train_list = os.listdir(ART_train_path)\n",
    "pre_train_list = os.listdir(PRE_train_path)\n",
    "art_test_list = os.listdir(ART_test_path)\n",
    "pre_test_list = os.listdir(PRE_test_path)\n",
    "\n",
    "\n",
    "# ART train\n",
    "for img in tqdm(art_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = ART_train_label.loc[ART_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(ART_train_path + img, new_ART_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(ART_train_path + img, new_ART_train_path + 'AML + onco/' + img)\n",
    "\n",
    "# PRE train\n",
    "for img in tqdm(pre_train_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_train_label.loc[PRE_train_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(PRE_train_path + img, new_PRE_train_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(PRE_train_path + img, new_PRE_train_path + 'AML + onco/' + img)\n",
    "        \n",
    "# ART test\n",
    "for img in tqdm(art_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = ART_val_label.loc[ART_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(ART_test_path + img, new_ART_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(ART_test_path + img, new_ART_test_path + 'AML + onco/' + img)\n",
    "        \n",
    "# PRE test\n",
    "for img in tqdm(pre_test_list): \n",
    "    case_id = img[0:10]\n",
    "    label = PRE_val_label.loc[PRE_val_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        shutil.copy(PRE_test_path + img, new_PRE_test_path + 'RCC/' + img)\n",
    "    if label == ['AML + onco']: \n",
    "        shutil.copy(PRE_test_path + img, new_PRE_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check len of each labels\n",
    "\n",
    "print(\"ART Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"ART Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_ART_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_ART_test_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Train : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_train_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_train_path + 'AML + onco/')))\n",
    "print(\"----------\")\n",
    "print(\"PRE Test : \")\n",
    "print(\"RCC \", len(os.listdir(new_PRE_test_path + 'RCC/')))\n",
    "print(\"AML + onco \", len(os.listdir(new_PRE_test_path + 'AML + onco/')))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification - with configuration image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,ResNet101, VGG19, DenseNet201,   MobileNetV2 # ,EfficientNetB4\n",
    "from tensorflow.keras.applications import resnet, vgg16 , vgg19, densenet,  mobilenet_v2 # ,efficientnet\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus =tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    # tensorflow가 첫번째 gpu만 사용하도록 제한\n",
    "    try: \n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.is_gpu_available()\n",
    "#tf.test.is_built_with_cuda()\n",
    "tf.test.is_built_with_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 512\n",
    "img_width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TEST/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TRAIN/PRE/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/TEST/PRE/'\n",
    "\n",
    "image_shape = (512, 512, 3)\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                       rescale= 1./255.,\n",
    "                                       validation_split = 0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(dtype='float32', \n",
    "                                       rescale= 1./255.)\n",
    "\n",
    "ART_train_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "ART_val_generator = train_datagen.flow_from_directory(ART_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "ART_test_generator = test_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "                                                    \n",
    "\n",
    "PRE_train_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'training')\n",
    "PRE_val_generator = train_datagen.flow_from_directory(PRE_train_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical', \n",
    "                                                    subset = 'validation')\n",
    "PRE_test_generator = test_datagen.flow_from_directory(PRE_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res_model = ResNet50(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "for layer in res_model.layers:\n",
    "    if 'conv5' not in layer.name:\n",
    "        layer.trainable = False\n",
    "# Check if all layers except conv5 layers are not trainable\n",
    "#for i, layer in enumerate(res_model.layers):\n",
    "#    print(i, layer.name, \"-\", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "model = Sequential()\n",
    "model.add(res_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART train\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_ART-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.isgpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_res.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_res.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = tf.keras.models.load_model('./cancer_configuration_ART-ResNet50.hdf5')\n",
    "model_test.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE train\n",
    "checkpointer = ModelCheckpoint(filepath='./cancer_configuration_PRE-ResNet50.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "history_res = model.fit(PRE_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 100,\n",
    "                    verbose = 1,\n",
    "                    validation_data = PRE_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res.history['acc'], label = 'train',)\n",
    "plt.plot(history_res.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(300))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART Train\n",
    "optimizer = optimizers.Adam(learning_rate= 0.00001, decay= 1e-6)\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "checkpointer = ModelCheckpoint(filepath='./ResNet101_ART_epoch30_MassOnly.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)\n",
    "\n",
    "history_res101 = model.fit(ART_train_generator,\n",
    "                    steps_per_epoch = 20,\n",
    "                    epochs = 30,\n",
    "                    verbose = 1,\n",
    "                    validation_data = ART_val_generator,\n",
    "                    callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res101.history['acc'], label = 'train',)\n",
    "plt.plot(history_res101.history['val_acc'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Layer, Flatten\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "N_CLASSES = 2\n",
    "base_model = VGG16(include_top = False,\n",
    "                   weights = 'imagenet',\n",
    "                   input_shape = (img_size,img_size,3))\n",
    "\n",
    "last = base_model.layers[-2].output\n",
    "x = GlobalAveragePooling2D()(last)\n",
    "x = Dense(512,'relu')(x)\n",
    "x = Dense(N_CLASSES, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vgg16_ART.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(ART_train_generator,\n",
    "                    epochs=100,\n",
    "                    validation_data=ART_val_generator,\n",
    "                    callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "N_CLASSES = 2\n",
    "base_model = InceptionResNetV2(include_top = False,\n",
    "                   weights = 'imagenet',\n",
    "                   input_shape = (img_size,img_size,3))\n",
    "\n",
    "last = base_model.layers[-2].output\n",
    "x = GlobalAveragePooling2D()(last)\n",
    "x = Dense(512,'relu')(x)\n",
    "x = Dense(N_CLASSES, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"InceptionResNetV2_ART_epoch30.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(ART_train_generator,\n",
    "                    epochs=30,\n",
    "                    validation_data=ART_val_generator,\n",
    "                    callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('InceptionResNetV2_ART_epoch30.h5')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(GCCT_test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf.keras.applications.efficientnet import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation         \n",
    "change image data directory format to perform cross validation        \n",
    "모델은 ResNet101 을 사용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of file list\n",
    "# 라벨별로 분리해둔 폴더에서 filename - label 쌍의 dataframe을 생성\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_classified/TRAIN/ART/'\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_classified/TEST/ART/'\n",
    "# PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/PRE/'\n",
    "# PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "# GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TRAIN/GCCT/'\n",
    "# GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/GCCT/'\n",
    "\n",
    "# ART_train filename + label DF\n",
    "ART_train_RCC = pd.DataFrame(os.listdir(ART_train_path + 'RCC/'))\n",
    "ART_train_RCC['label'] = 'RCC'\n",
    "ART_train_RCC.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "ART_train_AMLonco = pd.DataFrame(os.listdir(ART_train_path + 'AML + onco/'))\n",
    "ART_train_AMLonco['label'] = 'AML + onco'\n",
    "ART_train_AMLonco.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "ART_train_df = pd.concat([ART_train_RCC, ART_train_AMLonco], axis = 0)\n",
    "ART_train_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # PRE_train filename + label DF\n",
    "# PRE_train_RCC = pd.DataFrame(os.listdir(PRE_train_path + 'RCC/'))\n",
    "# PRE_train_RCC['label'] = 'RCC'\n",
    "# PRE_train_RCC.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "# PRE_train_AMLonco = pd.DataFrame(os.listdir(PRE_train_path + 'AML + onco/'))\n",
    "# PRE_train_AMLonco['label'] = 'AML + onco'\n",
    "# PRE_train_AMLonco.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "# PRE_train_df = pd.concat([PRE_train_RCC, PRE_train_AMLonco], axis = 0)\n",
    "# PRE_train_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # GCCT_train filename + label DF\n",
    "# GCCT_train_RCC = pd.DataFrame(os.listdir(GCCT_train_path + 'RCC/'))\n",
    "# GCCT_train_RCC['label'] = 'RCC'\n",
    "# GCCT_train_RCC.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "# GCCT_train_AMLonco = pd.DataFrame(os.listdir(GCCT_train_path + 'AML + onco/'))\n",
    "# GCCT_train_AMLonco['label'] = 'AML + onco'\n",
    "# GCCT_train_AMLonco.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "\n",
    "# GCCT_train_df = pd.concat([GCCT_train_RCC, GCCT_train_AMLonco], axis = 0)\n",
    "# GCCT_train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 합쳐진 데이터 다시 나누기\n",
    "\n",
    "ART_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/ART/'\n",
    "PRE_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/PRE/'\n",
    "GCCT_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_mass_total/GCCT/'\n",
    "\n",
    "new_ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/ART/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TEST/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/PRE/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TEST/PRE/'\n",
    "new_GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/GCCT/'\n",
    "new_GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TEST/GCCT/'\n",
    "\n",
    "# make dir\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/'\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"GCCT\"))\n",
    "\n",
    "\n",
    "# create img name list\n",
    "art_list = os.listdir(ART_path)\n",
    "pre_list = os.listdir(PRE_path)\n",
    "gcct_list = os.listdir(GCCT_path)\n",
    "\n",
    "\n",
    "# ART\n",
    "for img in tqdm(art_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        shutil.copy(ART_path + img, new_ART_train_path + img)\n",
    "    else: # test\n",
    "        shutil.copy(ART_path + img, new_ART_test_path + img)\n",
    "\n",
    "# PRE\n",
    "for img in tqdm(pre_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        shutil.copy(PRE_path + img, new_PRE_train_path + img)\n",
    "    else: # test\n",
    "        shutil.copy(PRE_path + img, new_PRE_test_path + img)\n",
    "        \n",
    "# GCCT\n",
    "for img in tqdm(gcct_list): \n",
    "    case_id = img[0:10]\n",
    "    if case_id[6:10] <= '0320': # train\n",
    "        shutil.copy(GCCT_path + img, new_GCCT_train_path + img)\n",
    "    else: # test\n",
    "        shutil.copy(GCCT_path + img, new_GCCT_test_path + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_shape = (512, 512, 3)\n",
    "# res101_model = ResNet101(include_top=False, pooling='avg', weights='imagenet', input_shape = (image_shape))\n",
    "# for layer in res101_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512, 512, 3)\n",
    "res101_model = ResNet101(include_top=False, pooling='avg', weights=None, input_shape = (image_shape))\n",
    "for layer in res101_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 정의 - ResNet101\n",
    "N_CLASSES = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(res101_model)\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(300))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gradcam 적용 위해서 VGG16으로 바꿔보기\n",
    "\n",
    "base_model = VGG16(include_top = False,\n",
    "                   weights = 'imagenet',\n",
    "                   input_shape = (img_size,img_size,3))\n",
    "\n",
    "last = base_model.layers[-2].output\n",
    "x = GlobalAveragePooling2D()(last)\n",
    "x = Dense(512,'relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "\n",
    "# ART_test_path = '/home/ncp/workspace/blocks3/zio_code/kidneyData_windowing/TEST/ART/'\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# # create datagen\n",
    "# create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# # generate test data first\n",
    "# ART_test_generator = create_datagen.flow_from_directory(ART_test_path,\n",
    "#                                                    batch_size = BATCH_SIZE,\n",
    "#                                                    target_size = (512, 512),\n",
    "#                                                    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART # - TRAIN: 서울대, TEST: 부산+보라매일 경우\n",
    "# 다시 라벨별 분리하지 않은 데이터 경로에서 작업 (덕선선생님이 만드신것)\n",
    "# test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "# 1007 learning rate reduction 추가\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/PRE/'\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/GCCT/'\n",
    "\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "ART_test_generator = create_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "VALIDATION_AUC = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(ART_train_df, ART_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = ART_train_df.iloc[train_index]\n",
    "    validation_data = ART_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'AUC'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./ResNet101_crossval_ART_epoch30.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        epochs = 3,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    VALIDATION_AUC.append(results['auc'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat json label dataframes\n",
    "\n",
    "ART_label = pd.concat([ART_train_label, ART_val_label], axis = 0).reset_index(drop = True)\n",
    "PRE_label = pd.concat([PRE_train_label, PRE_val_label], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/')\n",
    "shutil.rmtree('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TEST/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN+TEST 합쳐진 데이터에서 랜덤하게 20%만 분리해서 train_shuffle, test_shuffle로 다시 저장\n",
    "# Train: ART, PRE, GCCT 만 분리하고, 안에 라벨은 생성 x (label df를 따로 사용)\n",
    "# Test: 20% 분리 후 라벨까지 생성\n",
    "\n",
    "ART_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly/ART/'\n",
    "PRE_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly/PRE/'\n",
    "# GCCT_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massBoundingBox/GCCT/'\n",
    "\n",
    "new_ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/ART/'\n",
    "new_PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/PRE/'\n",
    "# new_GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/GCCT/'\n",
    "new_ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TEST/ART/'\n",
    "new_PRE_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TEST/PRE/'\n",
    "# new_GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/GCCT/'\n",
    "\n",
    "# make dir\n",
    "def makedir(path): \n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)\n",
    "        \n",
    "dir = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/'\n",
    "makedir(os.path.join(dir, \"TRAIN\"))\n",
    "makedir(os.path.join(dir, \"TEST\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TRAIN\", \"GCCT\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"ART\", \"AML + onco\"))\n",
    "\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"RCC\"))\n",
    "makedir(os.path.join(dir, \"TEST\", \"PRE\", \"AML + onco\"))\n",
    "\n",
    "# makedir(os.path.join(dir, \"TEST\", \"GCCT\"))\n",
    "# makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"RCC\"))\n",
    "# makedir(os.path.join(dir, \"TEST\", \"GCCT\", \"AML + onco\"))\n",
    "\n",
    "\n",
    "# create img name list\n",
    "art_list = os.listdir(ART_path)\n",
    "pre_list = os.listdir(PRE_path)\n",
    "# gcct_list = os.listdir(GCCT_path)\n",
    "\n",
    "\n",
    "# split 20% and create train, test list\n",
    "import random\n",
    "# ART\n",
    "art_rcc_list = os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/ART/RCC/')\n",
    "art_amlonco_list = os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/ART/AML + onco/')\n",
    "\n",
    "art_rcc_test_list = random.sample(art_rcc_list, int(round(len(art_rcc_list) * 0.2)))\n",
    "art_amlonco_test_list = random.sample(art_amlonco_list, int(round(len(art_amlonco_list) * 0.2)))\n",
    "\n",
    "art_test_list = list(set(art_rcc_test_list+art_amlonco_test_list))\n",
    "art_train_list = list(set(art_rcc_list)-set(art_rcc_test_list)) + list(set(art_amlonco_list)-set(art_amlonco_test_list))\n",
    "\n",
    "# PRE\n",
    "pre_rcc_list = os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/PRE/RCC/')\n",
    "pre_amlonco_list = os.listdir('/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_classified/PRE/AML + onco/')\n",
    "\n",
    "pre_rcc_test_list = random.sample(pre_rcc_list, int(round(len(pre_rcc_list) * 0.2)))\n",
    "pre_amlonco_test_list = random.sample(pre_amlonco_list, int(round(len(pre_amlonco_list) * 0.2)))\n",
    "\n",
    "pre_test_list = list(set(pre_rcc_test_list+pre_amlonco_test_list))\n",
    "pre_train_list = list(set(pre_rcc_list)-set(pre_rcc_test_list)) + list(set(pre_amlonco_list)-set(pre_amlonco_test_list))\n",
    "\n",
    "# # GCCT\n",
    "# gcct_rcc_list = os.listdir('/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_classified_Train+Test/GCCT/RCC/')\n",
    "# gcct_amlonco_list = os.listdir('/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_classified_Train+Test/GCCT/AML + onco/')\n",
    "\n",
    "# gcct_rcc_test_list = random.sample(gcct_rcc_list, int(round(len(gcct_rcc_list) * 0.2)))\n",
    "# gcct_amlonco_test_list = random.sample(gcct_amlonco_list, int(round(len(gcct_amlonco_list) * 0.2)))\n",
    "\n",
    "# gcct_test_list = list(set(gcct_rcc_test_list+gcct_amlonco_test_list))\n",
    "# gcct_train_list = list(set(gcct_rcc_list)-set(gcct_rcc_test_list)) + list(set(gcct_amlonco_list)-set(gcct_amlonco_test_list))\n",
    "\n",
    "\n",
    "# ART train\n",
    "for img in tqdm(art_list): \n",
    "    if img in art_train_list: \n",
    "        shutil.copy(ART_path + img, new_ART_train_path + img)\n",
    "\n",
    "# PRE train\n",
    "for img in tqdm(pre_list): \n",
    "    if img in pre_train_list: \n",
    "        shutil.copy(PRE_path + img, new_PRE_train_path + img)\n",
    "        \n",
    "# # GCCT train\n",
    "# for img in tqdm(gcct_list): \n",
    "#     if img in gcct_train_list: \n",
    "#         shutil.copy(GCCT_path + img, new_GCCT_train_path + img)\n",
    "        \n",
    "# ART test\n",
    "for img in tqdm(art_list): \n",
    "    if img in art_test_list: \n",
    "        case_id = img[0:10]\n",
    "        label = ART_label.loc[ART_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(ART_path + img, new_ART_test_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(ART_path + img, new_ART_test_path + 'AML + onco/' + img)\n",
    "\n",
    "# PRE test\n",
    "for img in tqdm(pre_list): \n",
    "    if img in pre_test_list: \n",
    "        case_id = img[0:10]\n",
    "        label = PRE_label.loc[PRE_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "        if label == ['RCC']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_test_path + 'RCC/' + img)\n",
    "        if label == ['AML + onco']: \n",
    "            shutil.copy(PRE_path + img, new_PRE_test_path + 'AML + onco/' + img)\n",
    "        \n",
    "# # GCCT test\n",
    "# for img in tqdm(gcct_list): \n",
    "#     if img in gcct_test_list: \n",
    "#         case_id = img[0:10]\n",
    "#         label = PRE_label.loc[PRE_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "#         if label == ['RCC']: \n",
    "#             shutil.copy(GCCT_path + img, new_GCCT_test_path + 'RCC/' + img)\n",
    "#         if label == ['AML + onco']: \n",
    "#             shutil.copy(GCCT_path + img, new_GCCT_test_path + 'AML + onco/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(art_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(art_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of file list\n",
    "# 라벨별로 분리해둔 폴더에서 filename - label 쌍의 dataframe을 생성\n",
    "# 1111 서울대+부산+보라매 합칠 시 실행 - 재 생성 필요함\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/PRE/'\n",
    "# GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/GCCT/'\n",
    "\n",
    "# ART_train filename + label DF\n",
    "ART_train_df = pd.DataFrame(os.listdir(ART_train_path))\n",
    "ART_train_df.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "ART_train_df['label'] = ''\n",
    "for row in range(len(ART_train_df)): \n",
    "    case_id = ART_train_df['filename'][row][0:10]\n",
    "    label = ART_label.loc[ART_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        ART_train_df['label'][row] = 'RCC'\n",
    "    if label == ['AML + onco']:\n",
    "        ART_train_df['label'][row] = 'AML + onco'\n",
    "        \n",
    "ART_train_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# PRE_train filename + label DF\n",
    "PRE_train_df = pd.DataFrame(os.listdir(PRE_train_path))\n",
    "PRE_train_df.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "PRE_train_df['label'] = ''\n",
    "for row in range(len(PRE_train_df)): \n",
    "    case_id = PRE_train_df['filename'][row][0:10]\n",
    "    label = PRE_label.loc[PRE_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "    if label == ['RCC']: \n",
    "        PRE_train_df['label'][row] = 'RCC'\n",
    "    if label == ['AML + onco']:\n",
    "        PRE_train_df['label'][row] = 'AML + onco'\n",
    "        \n",
    "PRE_train_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # GCCT_train filename + label DF\n",
    "# GCCT_train_df = pd.DataFrame(os.listdir(GCCT_train_path))\n",
    "# GCCT_train_df.rename(columns = {0 : 'filename'}, inplace = True)\n",
    "# GCCT_train_df['label'] = ''\n",
    "# for row in range(len(GCCT_train_df)): \n",
    "#     case_id = GCCT_train_df['filename'][row][0:10]\n",
    "#     label = PRE_label.loc[PRE_label['case_id'] == case_id, 'label'].unique().tolist()\n",
    "#     if label == ['RCC']: \n",
    "#         GCCT_train_df['label'][row] = 'RCC'\n",
    "#     if label == ['AML + onco']:\n",
    "#         GCCT_train_df['label'][row] = 'AML + onco'\n",
    "        \n",
    "# GCCT_train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ART # - 서울대+부산+보라매 다 합쳐서 random하게 train test split한 데이터로 train\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TRAIN/ART/'\n",
    "# PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/PRE/'\n",
    "# GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/GCCT/'\n",
    "\n",
    "ART_test_path = '/home/ncp/workspace/blocks1/kidneyData_windowing_massOnly_Shuffled_TrainTest/TEST/ART/'\n",
    "# PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/PRE/'\n",
    "# GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "ART_test_generator = create_datagen.flow_from_directory(ART_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "VALIDATION_AUC = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(ART_train_df, ART_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = ART_train_df.iloc[train_index]\n",
    "    validation_data = ART_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = ART_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'AUC'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./RES101_crossval_ART_epoch30_afterShuffle_MassOnly.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        epochs = 30,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    VALIDATION_AUC.append(results['auc'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_resnet_ART = '/home/ncp/workspace/blocks3/zio_code/cancer_configuration_ART-ResNet101_crossval.hdf5'\n",
    "# ART_resnet = tf.keras.models.load_model(path_resnet_ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(ART_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data first\n",
    "GCCT_test_generator = create_datagen.flow_from_directory(GCCT_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gcct = model.evaluate(GCCT_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(VALIDATION_ACCURACY, label = 'val_accuracy')\n",
    "plt.plot(VALIDATION_LOSS, label = 'val_loss')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Val acc and loss by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc\n",
    "\n",
    "plt.plot(VALIDATION_AUC, label = 'val_auc', color = 'g')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Val AUC by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grad cam 적용 ##\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Layer, Flatten\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = ART_test_generator.classes\n",
    "y_pred = model.predict(ART_test_generator)\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = ART_test_generator.class_indices\n",
    "indices = {v:k for k,v in class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ART_test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame()\n",
    "val_df['filename'] = filenames\n",
    "val_df['actual'] = y_val\n",
    "val_df['predicted'] = y_pred\n",
    "val_df['actual'] = val_df['actual'].apply(lambda x: indices[x])\n",
    "val_df['predicted'] = val_df['predicted'].apply(lambda x: indices[x])\n",
    "val_df.loc[val_df['actual']==val_df['predicted'],'Same'] = True\n",
    "val_df.loc[val_df['actual']!=val_df['predicted'],'Same'] = False\n",
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "def readImage(path):\n",
    "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def display_images(temp_df, image_type): # image_type: one of ART, PRE, GCCT\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)\n",
    "        image = readImage(f\"/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/{image_type}/{temp_df.filename[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'A: {temp_df.actual[i]} P: {temp_df.predicted[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctly classified\n",
    "\n",
    "display_images(val_df[val_df['Same']==True], 'ART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified\n",
    "display_images(val_df[val_df['Same']!=True], 'ART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, pred_index=None):\n",
    "    \n",
    "    grad_model = Model(inputs=model.inputs, outputs=[model.layers[-4].output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy(), preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCAMImage(image, image_type):\n",
    "    path = \"/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/{}/{}\".format(image_type, image)\n",
    "    img = readImage(path)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    heatmap,preds = make_gradcam_heatmap(img,model)\n",
    "\n",
    "    img = load_img(path)\n",
    "    img = img_to_array(img)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * 0.8 + img\n",
    "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    \n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_of_images(correct_class, image_type):\n",
    "    grad_images = []\n",
    "    title = []\n",
    "    temp_df = val_df[val_df['Same']==correct_class]\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    for i in range(15):\n",
    "        image = temp_df.filename[i]\n",
    "        grad_image = gradCAMImage(image, image_type)\n",
    "        grad_images.append(grad_image)\n",
    "        title.append(f\"A: {temp_df.actual[i]} P: {temp_df.predicted[i]}\")\n",
    "\n",
    "    return grad_images, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_classified, c_titles = gradcam_of_images(correct_class=True, 'ART')\n",
    "misclassified, m_titles = gradcam_of_images(correct_class=False, 'ART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_heatmaps(classified_images,titles):\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)\n",
    "        plt.imshow(classified_images[i])\n",
    "        plt.title(titles[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAD-CAM: Correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmaps(correctly_classified,c_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAD-CAM: Incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmaps(misclassified,m_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # roc curve - check if works\n",
    "\n",
    "# from sklearn import datasets\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "# X, y = X[y != 2], y[y != 2]\n",
    "# n_samples, n_features = X.shape\n",
    "\n",
    "# # Add noisy features\n",
    "# random_state = np.random.RandomState(0)\n",
    "# X = np.c_[X, random_state.randn(n_samples, 200*n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import auc\n",
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# # Run classifier with cross-validation and plot ROC curves\n",
    "# cv = StratifiedKFold(n_splits = 6)\n",
    "# classifier = svm.SVC(kernel = \"linear\", probability = True, random_state = random_state)\n",
    "\n",
    "# tprs = []\n",
    "# aucs = []\n",
    "# mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# for i, (train, test) in enumerate(cv.split(X, y)): \n",
    "#     viz = RocCurveDisplay.from_estimator(\n",
    "#     model, \n",
    "#     train_data_generator, \n",
    "#     valid_data_generator, \n",
    "#     name = \"ROC fold {}\".format(fold_var), \n",
    "#     alpha = 0.3, \n",
    "#     lw = 1, \n",
    "#     ax = ax\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # roc curve\n",
    "\n",
    "# acc = history_res101.history['accuracy']\n",
    "# val_acc = history_res101.history['val_accuracy']\n",
    "\n",
    "# AUC = history_res101.history['AUC']\n",
    "# val_AUC = history_res101.history['val_auc']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plt.plot(epochs, AUC, 'g', label = 'AUC')\n",
    "# plt.plot(epochs, val_AUC, 'b', label = 'Val_AUC')\n",
    "# plt.xlabel('No.of epochs')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.title('Training and validation AUC')\n",
    "# plt.legend()\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCCT\n",
    "# 다시 라벨별 분리하지 않은 데이터 경로에서 작업 (덕선선생님이 만드신것)\n",
    "# test dataset 은 다시 라벨별로 분리해둔 폴더를 사용\n",
    "# 1007 learning rate reduction 추가\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/PRE/'\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_traintest/TRAIN/GCCT/'\n",
    "\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/PRE/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_classified/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "GCCT_test_generator = create_datagen.flow_from_directory(GCCT_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "VALIDATION_AUC = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(GCCT_train_df, GCCT_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = GCCT_train_df.iloc[train_index]\n",
    "    validation_data = GCCT_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'AUC'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./ResNet101_crossval_GCCT_epoch30.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        epochs = 3,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    VALIDATION_AUC.append(results['auc'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GCCT # - 서울대, 보라매, 부산 다 합쳐서 random하게 split 한 데이터로 train\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TRAIN/PRE/'\n",
    "GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TRAIN/GCCT/'\n",
    "\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TEST/PRE/'\n",
    "GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_mass_total_Shuffled_TrainTest/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "GCCT_test_generator = create_datagen.flow_from_directory(GCCT_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "VALIDATION_AUC = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(GCCT_train_df, GCCT_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = GCCT_train_df.iloc[train_index]\n",
    "    validation_data = GCCT_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = GCCT_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'AUC'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./ResNet101_crossval_GCCT_epoch30_afterShuffle.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        epochs = 30,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    VALIDATION_AUC.append(results['auc'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(GCCT_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(VALIDATION_ACCURACY, label = 'val_accuracy')\n",
    "plt.plot(VALIDATION_LOSS, label = 'val_loss')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Val acc and loss by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc\n",
    "\n",
    "plt.plot(VALIDATION_AUC, label = 'val_auc', color = 'g')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Val AUC by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE # - 서울대+부산+보라매 다 합쳐서 random하게 train test split한 데이터로 train\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ART_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/ART/'\n",
    "PRE_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/PRE/'\n",
    "# GCCT_train_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TRAIN/GCCT/'\n",
    "\n",
    "ART_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/ART/'\n",
    "PRE_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/PRE/'\n",
    "# GCCT_test_path = '/home/ncp/workspace/blocks3/kidneyData_windowing_massBoundingBox_Shuffled_TrainTest/TEST/GCCT/'\n",
    "\n",
    "# y is imbalance - KFold 대신 StratifiedKFold 사용\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "\n",
    "# create datagen\n",
    "create_datagen = ImageDataGenerator(rescale= 1./255.)\n",
    "\n",
    "\n",
    "# generate test data first\n",
    "PRE_test_generator = create_datagen.flow_from_directory(PRE_test_path,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   target_size = (512, 512),\n",
    "                                                   class_mode = 'categorical')\n",
    "\n",
    "\n",
    "## PERFORM TRAIN & VAL ## - ART\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "VALIDATION_AUC = []\n",
    "val_acc = 0\n",
    "\n",
    "fold_var = 1 # weight 나 모델 저장용\n",
    "\n",
    "for train_index, val_index in skf.split(PRE_train_df, PRE_train_df['label']):  # Y의 분포 기준으로 나눔\n",
    "    \n",
    "    training_data = PRE_train_df.iloc[train_index]\n",
    "    validation_data = PRE_train_df.iloc[val_index]\n",
    "    \n",
    "    train_data_generator = create_datagen.flow_from_dataframe(training_data, directory = PRE_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    valid_data_generator = create_datagen.flow_from_dataframe(validation_data, directory = PRE_train_path, \n",
    "                                                             x_col = 'filename', y_col = 'label', \n",
    "                                                             class_mode = 'categorical', \n",
    "                                                             shuffle = True)\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate= 0.0001, decay= 1e-6)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           patience =3, \n",
    "                                           verbose =1,\n",
    "                                            factor = 0.2,\n",
    "                                            min_lr =0.0000001)\n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy', 'AUC'])\n",
    "    checkpointer = ModelCheckpoint(filepath='./VGG16_crossval_PRE_epoch30_afterShuffle_BoundingBoxAdded.hdf5',\n",
    "                            monitor='val_loss', verbose = 1,\n",
    "                            save_best_only=True)\n",
    "    early_stopping = EarlyStopping(verbose=1, patience=15)\n",
    "\n",
    "    history_res101 = model.fit(train_data_generator,\n",
    "                        epochs = 30,\n",
    "                        verbose = 1,\n",
    "                        validation_data = valid_data_generator,\n",
    "                        callbacks = [checkpointer, early_stopping, learning_rate_reduction])\n",
    "    \n",
    "    # validation accuracy, loss\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names, results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    VALIDATION_AUC.append(results['auc'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "    \n",
    "    val_acc += results['accuracy'] / 10   # 평균 ACC\n",
    "    \n",
    "print(\"mean validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(VALIDATION_ACCURACY, label = 'val_accuracy',)\n",
    "plt.plot(VALIDATION_LOSS, label = 'val_loss')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Val acc and loss by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc\n",
    "\n",
    "plt.plot(VALIDATION_AUC, label = 'val_auc', color = 'g')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('folds')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Val AUC by folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grad cam 적용 ##\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Layer, Flatten\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = PRE_test_generator.classes\n",
    "y_pred = model.predict(PRE_test_generator)\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = PRE_test_generator.class_indices\n",
    "indices = {v:k for k,v in class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = PRE_test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame()\n",
    "val_df['filename'] = filenames\n",
    "val_df['actual'] = y_val\n",
    "val_df['predicted'] = y_pred\n",
    "val_df['actual'] = val_df['actual'].apply(lambda x: indices[x])\n",
    "val_df['predicted'] = val_df['predicted'].apply(lambda x: indices[x])\n",
    "val_df.loc[val_df['actual']==val_df['predicted'],'Same'] = True\n",
    "val_df.loc[val_df['actual']!=val_df['predicted'],'Same'] = False\n",
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctly classified\n",
    "\n",
    "display_images(val_df[val_df['Same']==True], 'PRE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified\n",
    "display_images(val_df[val_df['Same']!=True], 'PRE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_classified, c_titles = gradcam_of_images(correct_class=True, image_type = 'PRE')\n",
    "misclassified, m_titles = gradcam_of_images(correct_class=False, image_type = 'PRE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAD-CAM: Correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmaps(correctly_classified,c_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAD-CAM: Incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmaps(misclassified,m_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_resnet_PRE = '/home/ncp/workspace/blocks3/zio_code/cancer_configuration_PRE-ResNet101_crossval.hdf5'\n",
    "PRE_resnet = tf.keras.models.load_model(path_resnet_PRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_PRE = model.evaluate(PRE_test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_res101_PRE.history['accuracy'], label = 'train',)\n",
    "plt.plot(history_res101_PRE.history['val_accuracy'], label = 'val')\n",
    "\n",
    "plt.legend(loc = 'right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
